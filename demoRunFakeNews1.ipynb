{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import necessary stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import en_core_web_lg\n",
    "import en_core_web_sm\n",
    "from contractions import contractions_dict\n",
    "\n",
    "import twint\n",
    "import csv\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "from textstat.textstat import textstatistics, easy_word_set, legacy_round\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import statsmodels.api as sm\n",
    "import statsmodels\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#python -m spacy download en\n",
    "#python -m spacy download en_core_web_sm\n",
    "#python -m spacy download en_core_web_lg\n",
    "\n",
    "## Packages used by script to extract tweets for the URLs: scriptExtractTweets1.py\n",
    "# import pandas as pd\n",
    "# import twint\n",
    "# import csv\n",
    "# #import os\n",
    "# import sys\n",
    "# from datetime import datetime\n",
    "# import logging\n",
    "\n",
    "## Packages used by script to create Readability features: scriptCreateReadabilityFeatures1.py\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import spacy\n",
    "# import nltk\n",
    "# from nltk.tokenize.toktok import ToktokTokenizer\n",
    "# import re\n",
    "# from bs4 import BeautifulSoup\n",
    "# import unicodedata\n",
    "# import en_core_web_lg\n",
    "# import en_core_web_sm\n",
    "# from contractions import contractions_dict\n",
    "# from textstat.textstat import textstatistics, easy_word_set, legacy_round\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# import csv\n",
    "# import sys\n",
    "# from datetime import datetime\n",
    "# import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', 500)\n",
    "pd.set_option('max_colwidth', -1)\n",
    "pd.set_option('max_rows', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input file with the URLs to classify\n",
    "## domain, url, urlType, title, content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "urlsFile4Demo = '/home/rohit/SRH/CaseStudy1/dataDemo/dataFiles/demorun_input_data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using twint extract the tweets that have the url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this script to process the datasets for FAKE, RELIABLE AND CONSPIRACY data. \n",
    "# Run the rogram with EXACTLY 5 argruments as shown below:\n",
    "#       python <script> <input-file-name> <output-CSV-file> <#skipInitialRows> <#rowsToProcess> <#rowsAfterWhichToShowPrintMessageTracker>\n",
    "#\n",
    "#       #skipInitialRows\n",
    "#               is an integer. Is the CSV data row number (0 being for header) from which the data\n",
    "#               should be loaded. Here 0 is the header so to skip first 5 data rows: skipInitialRows = 5.\n",
    "#       #rowsToProcess\n",
    "#               is an integer. If = -1 then will read in all the rows after skipping as per above parameter.\n",
    "#               Otherwise, will read in the number specified.\n",
    "## Note only if running in jupyter then uncomment the two lines for import nest_asyncio and the nest_asyncio.apply()\n",
    "#\n",
    "# The packages expected by the script are:\n",
    "# import pandas as pd\n",
    "# import twint\n",
    "# import csv\n",
    "# #import os\n",
    "# import sys\n",
    "# from datetime import datetime\n",
    "# import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All command line arguments are valid....starting main processing\n",
      "\n",
      "Start time: Tue Oct 22 10:34:55 2019\n",
      "\n",
      "\n",
      "Processing with command line arguments as:\n",
      "1) dataIpFile       = /home/rohit/SRH/CaseStudy1/dataDemo/dataFiles/demorun_input_data.csv\n",
      "2) csvOpFile        = /home/rohit/SRH/CaseStudy1/dataDemo/dataFiles/twint_data.csv\n",
      "3) skipInitialRows  = 0\n",
      "4) rowsToProcess    = 15\n",
      "5) printMessageFreq = 1\n",
      "\n",
      "\n",
      "Input data read into dataframe.\n",
      "\n",
      "\n",
      "Processing row number 1\n",
      "Number of rows skipped   = 0\n",
      "Number of rows processed = 1\n",
      "\n",
      "End time: Tue Oct 22 10:34:56 2019\n"
     ]
    }
   ],
   "source": [
    "!python /home/rohit/SRH/CaseStudy1/dataDemo/code/scriptExtractTweets1.py /home/rohit/SRH/CaseStudy1/dataDemo/dataFiles/demorun_input_data.csv /home/rohit/SRH/CaseStudy1/dataDemo/dataFiles/twint_data.csv 0 15 1\n",
    "#output file: /home/rohit/SRH/CaseStudy1/dataDemo/dataFiles/twint_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets have been extracted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T features -- extract TWITTER features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataIpFile = '/home/rohit/SRH/CaseStudy1/dataDemo/dataFiles/twint_data.csv'\n",
    "ipDataUsecolsList = [ 'date' , 'user_id_str' , 'nlikes' , 'nreplies' , 'nretweets' , 'search' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipInitialRows = 0\n",
    "dfIpData = pd.read_csv(dataIpFile, usecols = ipDataUsecolsList, skiprows = range(1, skipInitialRows + 1), sep = ',', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfIpData['domain'] = \"garbage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rohit/anaconda3/envs/fakenewsdemo1/lib/python3.6/site-packages/ipykernel_launcher.py:209: FutureWarning: Passing integers to fillna is deprecated, will raise a TypeError in a future version.  To retain the old behavior, pass pd.Timedelta(seconds=n) instead.\n",
      "/home/rohit/anaconda3/envs/fakenewsdemo1/lib/python3.6/site-packages/ipykernel_launcher.py:214: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/rohit/anaconda3/envs/fakenewsdemo1/lib/python3.6/site-packages/ipykernel_launcher.py:215: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/rohit/anaconda3/envs/fakenewsdemo1/lib/python3.6/site-packages/ipykernel_launcher.py:324: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/rohit/anaconda3/envs/fakenewsdemo1/lib/python3.6/site-packages/pandas/core/indexing.py:494: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n",
      "/home/rohit/anaconda3/envs/fakenewsdemo1/lib/python3.6/site-packages/ipykernel_launcher.py:352: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/rohit/anaconda3/envs/fakenewsdemo1/lib/python3.6/site-packages/ipykernel_launcher.py:354: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/rohit/anaconda3/envs/fakenewsdemo1/lib/python3.6/site-packages/ipykernel_launcher.py:355: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/rohit/anaconda3/envs/fakenewsdemo1/lib/python3.6/site-packages/ipykernel_launcher.py:356: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/rohit/anaconda3/envs/fakenewsdemo1/lib/python3.6/site-packages/ipykernel_launcher.py:357: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/rohit/anaconda3/envs/fakenewsdemo1/lib/python3.6/site-packages/ipykernel_launcher.py:358: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/rohit/anaconda3/envs/fakenewsdemo1/lib/python3.6/site-packages/ipykernel_launcher.py:371: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/rohit/anaconda3/envs/fakenewsdemo1/lib/python3.6/site-packages/ipykernel_launcher.py:372: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/rohit/anaconda3/envs/fakenewsdemo1/lib/python3.6/site-packages/ipykernel_launcher.py:373: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/rohit/anaconda3/envs/fakenewsdemo1/lib/python3.6/site-packages/ipykernel_launcher.py:374: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/rohit/anaconda3/envs/fakenewsdemo1/lib/python3.6/site-packages/ipykernel_launcher.py:375: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/rohit/anaconda3/envs/fakenewsdemo1/lib/python3.6/site-packages/ipykernel_launcher.py:376: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/rohit/anaconda3/envs/fakenewsdemo1/lib/python3.6/site-packages/ipykernel_launcher.py:378: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/rohit/anaconda3/envs/fakenewsdemo1/lib/python3.6/site-packages/pandas/core/frame.py:3498: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.loc._setitem_with_indexer((slice(None), indexer), value)\n",
      "/home/rohit/anaconda3/envs/fakenewsdemo1/lib/python3.6/site-packages/pandas/core/frame.py:3469: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_array(key, value)\n",
      "/home/rohit/anaconda3/envs/fakenewsdemo1/lib/python3.6/site-packages/pandas/core/indexing.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n",
      "/home/rohit/anaconda3/envs/fakenewsdemo1/lib/python3.6/site-packages/ipykernel_launcher.py:379: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/rohit/anaconda3/envs/fakenewsdemo1/lib/python3.6/site-packages/ipykernel_launcher.py:380: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/rohit/anaconda3/envs/fakenewsdemo1/lib/python3.6/site-packages/ipykernel_launcher.py:381: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "dfIpData['dateNew'] = pd.to_datetime(dfIpData['date'])\n",
    "dfOut = dfIpData[['search', 'domain']]\n",
    "dfOut = dfOut.drop_duplicates(subset = ['search'], keep = 'first')\n",
    "dfOut = dfOut.reset_index(drop=True)\n",
    "dfOut['urlType'] = \"garbage\"\n",
    "\n",
    "####################################################\n",
    "## number of tweets  (n)\n",
    "dfIpTemp = dfIpData.copy()\n",
    "dfIpTemp['totalTweets'] = dfIpTemp['search'].map(dfIpTemp['search'].value_counts())\n",
    "dfIpTemp = dfIpTemp.drop_duplicates(subset=['search'], keep='first')\n",
    "dfIpTemp = dfIpTemp.reset_index(drop=True)\n",
    "dfOut['totalTweets'] = dfIpTemp['totalTweets']\n",
    "del(dfIpTemp)\n",
    "\n",
    "####################################################\n",
    "## totalRetweets, totalLikes, totalReplies\n",
    "dfIpTemp = dfIpData.copy()\n",
    "dfIpTemp['totalRetweets'] = dfIpTemp.groupby('search')['nretweets'].transform('sum')\n",
    "dfIpTemp = dfIpTemp.drop_duplicates(subset=['search'], keep='first')\n",
    "dfIpTemp = dfIpTemp.reset_index(drop=True)\n",
    "dfOut['totalRetweets'] =  dfIpTemp['totalRetweets']\n",
    "del(dfIpTemp)\n",
    "#\n",
    "dfIpTemp = dfIpData.copy()\n",
    "dfIpTemp['totalLikes'] = dfIpTemp.groupby('search')['nlikes'].transform('sum')\n",
    "dfIpTemp = dfIpTemp.drop_duplicates(subset=['search'], keep='first')\n",
    "dfIpTemp = dfIpTemp.reset_index(drop=True)\n",
    "dfOut['totalLikes'] =  dfIpTemp['totalLikes']\n",
    "del(dfIpTemp)\n",
    "#\n",
    "dfIpTemp = dfIpData.copy()\n",
    "dfIpTemp['totalReplies'] = dfIpTemp.groupby('search')['nreplies'].transform('sum')\n",
    "dfIpTemp = dfIpTemp.drop_duplicates(subset=['search'], keep='first')\n",
    "dfIpTemp = dfIpTemp.reset_index(drop=True)\n",
    "dfOut['totalReplies'] =  dfIpTemp['totalReplies']\n",
    "del(dfIpTemp)\n",
    "#\n",
    "\n",
    "####################################################\n",
    "## span\n",
    "dfIpTemp = dfIpData.groupby('search', sort=False, as_index=False).apply(pd.DataFrame.sort_values, 'dateNew')\n",
    "dfOutTemp = dfIpTemp.groupby('search')['dateNew'].agg(['min', 'max'])\n",
    "del(dfIpTemp)\n",
    "dfOutTemp.reset_index(inplace = True)\n",
    "dfOutTemp['max'] = pd.to_datetime(dfOutTemp['max'])\n",
    "dfOutTemp['min'] = pd.to_datetime(dfOutTemp['min'])\n",
    "dfOutTemp['span'] = (dfOutTemp['max'] - dfOutTemp['min']) / np.timedelta64(1, 'h') \n",
    "dfOut.insert(7, 'span', dfOut['search'].map(dfOutTemp.set_index('search')['span']))\n",
    "dfOut.loc[dfOut['span'] == 0, 'span'] = -1\n",
    "del(dfOutTemp)\n",
    "\n",
    "####################################################\n",
    "# date processing to find out weekday, weekend, day of week counts\n",
    "## process Weekday counts\n",
    "dfIpTemp = dfIpData.copy()\n",
    "#\n",
    "dfIpTemp['tempDayOfWeek'] = dfIpTemp['dateNew'].dt.day_name()\n",
    "#\n",
    "dfIpTemp.loc[((dfIpTemp['tempDayOfWeek'] == 'Saturday') | (dfIpTemp['tempDayOfWeek'] == 'Sunday')), 'tempWkdayWkendType'] = 'Weekend'\n",
    "dfIpTemp.loc[dfIpTemp['tempWkdayWkendType'] != 'Weekend', 'tempWkdayWkendType'] = 'Weekday'\n",
    "#\n",
    "dfIpTemp.loc[dfIpTemp['tempWkdayWkendType'] == 'Weekday', 'countWeekDay'] = 1\n",
    "dfIpTemp['countWeekDay'] = dfIpTemp['countWeekDay'].fillna(0)\n",
    "#\n",
    "dfIpTemp['countWeekDay'] = dfIpTemp.groupby('search')['countWeekDay'].transform('sum')\n",
    "dfIpTemp = dfIpTemp.drop_duplicates(subset=['search'], keep='first')\n",
    "dfIpTemp = dfIpTemp.reset_index(drop=True)\n",
    "#\n",
    "dfOut['countWeekDay'] = dfIpTemp['countWeekDay']\n",
    "#\n",
    "del(dfIpTemp)\n",
    "#\n",
    "## process Weekend counts\n",
    "dfIpTemp = dfIpData.copy()\n",
    "#\n",
    "dfIpTemp['tempDayOfWeek'] = dfIpTemp['dateNew'].dt.day_name()\n",
    "#\n",
    "dfIpTemp.loc[((dfIpTemp['tempDayOfWeek'] == 'Saturday') | (dfIpTemp['tempDayOfWeek'] == 'Sunday')), 'tempWkdayWkendType'] = 'Weekend'\n",
    "dfIpTemp.loc[dfIpTemp['tempWkdayWkendType'] != 'Weekend', 'tempWkdayWkendType'] = 'Weekday'\n",
    "#\n",
    "dfIpTemp.loc[dfIpTemp['tempWkdayWkendType'] == 'Weekend', 'countWeekEnd'] = 1\n",
    "dfIpTemp['countWeekEnd'] = dfIpTemp['countWeekEnd'].fillna(0)\n",
    "#\n",
    "dfIpTemp['countWeekEnd']   = dfIpTemp.groupby('search')['countWeekEnd'].transform('sum')\n",
    "dfIpTemp = dfIpTemp.drop_duplicates(subset=['search'], keep='first')\n",
    "dfIpTemp = dfIpTemp.reset_index(drop=True)\n",
    "#\n",
    "dfOut['countWeekEnd'] = dfIpTemp['countWeekEnd']\n",
    "#\n",
    "del(dfIpTemp)\n",
    "#\n",
    "## process Monday counts\n",
    "dfIpTemp = dfIpData.copy()\n",
    "#\n",
    "dfIpTemp['tempDayOfWeek'] = dfIpTemp['dateNew'].dt.day_name()\n",
    "#\n",
    "dfIpTemp.loc[dfIpTemp['tempDayOfWeek'] == 'Monday', 'countMonday'] = 1\n",
    "dfIpTemp['countMonday'] = dfIpTemp['countMonday'].fillna(0)\n",
    "#\n",
    "dfIpTemp['countMonday'] = dfIpTemp.groupby('search')['countMonday'].transform('sum')\n",
    "dfIpTemp = dfIpTemp.drop_duplicates(subset=['search'], keep='first')\n",
    "dfIpTemp = dfIpTemp.reset_index(drop=True)\n",
    "#\n",
    "dfOut['countMonday'] = dfIpTemp['countMonday']\n",
    "#\n",
    "del(dfIpTemp)\n",
    "#\n",
    "## process Tuesday counts\n",
    "dfIpTemp = dfIpData.copy()\n",
    "#\n",
    "dfIpTemp['tempDayOfWeek'] = dfIpTemp['dateNew'].dt.day_name()\n",
    "#\n",
    "dfIpTemp.loc[dfIpTemp['tempDayOfWeek'] == 'Tuesday', 'countTuesday'] = 1\n",
    "dfIpTemp['countTuesday'] = dfIpTemp['countTuesday'].fillna(0)\n",
    "#\n",
    "dfIpTemp['countTuesday'] = dfIpTemp.groupby('search')['countTuesday'].transform('sum')\n",
    "dfIpTemp = dfIpTemp.drop_duplicates(subset=['search'], keep='first')\n",
    "dfIpTemp = dfIpTemp.reset_index(drop=True)\n",
    "#\n",
    "dfOut['countTuesday'] = dfIpTemp['countTuesday']\n",
    "#\n",
    "del(dfIpTemp)\n",
    "#\n",
    "## process Wednesday counts\n",
    "dfIpTemp = dfIpData.copy()\n",
    "#\n",
    "dfIpTemp['tempDayOfWeek'] = dfIpTemp['dateNew'].dt.day_name()\n",
    "#\n",
    "dfIpTemp.loc[dfIpTemp['tempDayOfWeek'] == 'Wednesday', 'countWednesday'] = 1\n",
    "dfIpTemp['countWednesday'] = dfIpTemp['countWednesday'].fillna(0)\n",
    "#\n",
    "dfIpTemp['countWednesday'] = dfIpTemp.groupby('search')['countWednesday'].transform('sum')\n",
    "dfIpTemp = dfIpTemp.drop_duplicates(subset=['search'], keep='first')\n",
    "dfIpTemp = dfIpTemp.reset_index(drop=True)\n",
    "#\n",
    "dfOut['countWednesday'] = dfIpTemp['countWednesday']\n",
    "#\n",
    "del(dfIpTemp)\n",
    "#\n",
    "## process Thursday counts\n",
    "dfIpTemp = dfIpData.copy()\n",
    "#\n",
    "dfIpTemp['tempDayOfWeek'] = dfIpTemp['dateNew'].dt.day_name()\n",
    "#\n",
    "dfIpTemp.loc[dfIpTemp['tempDayOfWeek'] == 'Thursday', 'countThursday'] = 1\n",
    "dfIpTemp['countThursday'] = dfIpTemp['countThursday'].fillna(0)\n",
    "#\n",
    "dfIpTemp['countThursday'] = dfIpTemp.groupby('search')['countThursday'].transform('sum')\n",
    "dfIpTemp = dfIpTemp.drop_duplicates(subset=['search'], keep='first')\n",
    "dfIpTemp = dfIpTemp.reset_index(drop=True)\n",
    "#\n",
    "dfOut['countThursday'] = dfIpTemp['countThursday']\n",
    "#\n",
    "del(dfIpTemp)\n",
    "#\n",
    "## process Friday counts\n",
    "dfIpTemp = dfIpData.copy()\n",
    "#\n",
    "dfIpTemp['tempDayOfWeek'] = dfIpTemp['dateNew'].dt.day_name()\n",
    "#\n",
    "dfIpTemp.loc[dfIpTemp['tempDayOfWeek'] == 'Friday', 'countFriday'] = 1\n",
    "dfIpTemp['countFriday'] = dfIpTemp['countFriday'].fillna(0)\n",
    "#\n",
    "dfIpTemp['countFriday'] = dfIpTemp.groupby('search')['countFriday'].transform('sum')\n",
    "dfIpTemp = dfIpTemp.drop_duplicates(subset=['search'], keep='first')\n",
    "dfIpTemp = dfIpTemp.reset_index(drop=True)\n",
    "#\n",
    "dfOut['countFriday'] = dfIpTemp['countFriday']\n",
    "#\n",
    "del(dfIpTemp)\n",
    "#\n",
    "## process Saturday counts\n",
    "dfIpTemp = dfIpData.copy()\n",
    "#\n",
    "dfIpTemp['tempDayOfWeek'] = dfIpTemp['dateNew'].dt.day_name()\n",
    "#\n",
    "dfIpTemp.loc[dfIpTemp['tempDayOfWeek'] == 'Saturday', 'countSaturday'] = 1\n",
    "dfIpTemp['countSaturday'] = dfIpTemp['countSaturday'].fillna(0)\n",
    "#\n",
    "dfIpTemp['countSaturday'] = dfIpTemp.groupby('search')['countSaturday'].transform('sum')\n",
    "dfIpTemp = dfIpTemp.drop_duplicates(subset=['search'], keep='first')\n",
    "dfIpTemp = dfIpTemp.reset_index(drop=True)\n",
    "#\n",
    "dfOut['countSaturday'] = dfIpTemp['countSaturday']\n",
    "#\n",
    "del(dfIpTemp)\n",
    "#\n",
    "## process Sunday counts\n",
    "dfIpTemp = dfIpData.copy()\n",
    "#\n",
    "dfIpTemp['tempDayOfWeek'] = dfIpTemp['dateNew'].dt.day_name()\n",
    "#\n",
    "dfIpTemp.loc[dfIpTemp['tempDayOfWeek'] == 'Sunday', 'countSunday'] = 1\n",
    "dfIpTemp['countSunday'] = dfIpTemp['countSunday'].fillna(0)\n",
    "#\n",
    "dfIpTemp['countSunday'] = dfIpTemp.groupby('search')['countSunday'].transform('sum')\n",
    "dfIpTemp = dfIpTemp.drop_duplicates(subset=['search'], keep='first')\n",
    "dfIpTemp = dfIpTemp.reset_index(drop=True)\n",
    "#\n",
    "dfOut['countSunday'] = dfIpTemp['countSunday']\n",
    "#\n",
    "del(dfIpTemp)\n",
    "#\n",
    "\n",
    "####################################################\n",
    "## average time between tweets\n",
    "dfIpTemp = dfIpData.groupby('search', sort=False, as_index=False).apply(pd.DataFrame.sort_values, 'dateNew')\n",
    "dfIpTemp['timeBetween2Tweets'] = dfIpTemp.groupby('search')['dateNew'].diff().fillna(0)\n",
    "dfIpTemp['timeBetween2Tweets'] = dfIpTemp['timeBetween2Tweets'] / np.timedelta64(1, 'h')\n",
    "## using sum and dividing by n-1\n",
    "dfOutTemp = dfIpData[['search']]\n",
    "dfIpTemp = dfIpTemp.reset_index(drop=True)\n",
    "dfOutTemp['avgTimeBetweenTweets'] = dfIpTemp.groupby('search')['timeBetween2Tweets'].transform('sum')\n",
    "dfOutTemp['countTweet'] = dfOutTemp['search'].map(dfOutTemp['search'].value_counts())\n",
    "dfOutTemp = dfOutTemp.drop_duplicates(subset=['search'], keep='first')\n",
    "dfOutTemp = dfOutTemp.reset_index(drop=True)\n",
    "dfOutTemp['avgTimeBetweenTweets'] = dfOutTemp['avgTimeBetweenTweets'] / (dfOutTemp['countTweet'] - 1)\n",
    "dfOutTemp.loc[dfOutTemp['avgTimeBetweenTweets'].isnull(), 'avgTimeBetweenTweets'] = -1\n",
    "dfOut['avgTimeBetweenTweets'] = dfOutTemp['avgTimeBetweenTweets']\n",
    "del(dfOutTemp)\n",
    "del(dfIpTemp)\n",
    "\n",
    "####################################################\n",
    "## average time of next tweet\n",
    "dfIpTemp = dfIpData.groupby('search', sort=False, as_index=False).apply(pd.DataFrame.sort_values, 'dateNew')\n",
    "dfIpTemp['timeFromFirstTweet'] = dfIpTemp['dateNew'] - dfIpTemp.groupby('search')['dateNew'].transform('first')\n",
    "dfIpTemp['timeFromFirstTweet'] = dfIpTemp['timeFromFirstTweet']/np.timedelta64(1, 'h')\n",
    "dfIpTemp['countTweets'] = dfIpTemp['search'].map(dfIpTemp['search'].value_counts())\n",
    "dfIpTemp['avgTimeOfNextTweet'] = dfIpTemp.groupby('search')['timeFromFirstTweet'].transform(sum)\n",
    "dfIpTemp['avgTimeOfNextTweet'] = dfIpTemp['avgTimeOfNextTweet']/(dfIpTemp['countTweets'] - 1)\n",
    "dfIpTemp = dfIpTemp.drop_duplicates(subset = ['search'], keep = 'first')\n",
    "dfIpTemp.loc[dfIpTemp['avgTimeOfNextTweet'].isnull(), 'avgTimeOfNextTweet'] = -1\n",
    "dfIpTemp = dfIpTemp.reset_index(drop=True)\n",
    "dfOut = dfOut.reset_index(drop=True)\n",
    "dfOut['avgTimeOfNextTweet'] = dfIpTemp['avgTimeOfNextTweet']\n",
    "del(dfIpTemp)\n",
    "\n",
    "####################################################\n",
    "## Time Absolute Bins\n",
    "dfIpTemp = dfIpData.groupby('search', sort=False, as_index=False).apply(pd.DataFrame.sort_values, 'dateNew')\n",
    "dfIpTemp['timeFromFirstTweet'] = dfIpTemp['dateNew'] - dfIpTemp.groupby('search')['dateNew'].transform('first')\n",
    "dfIpTemp['timeFromFirstTweet'] = dfIpTemp['timeFromFirstTweet']/np.timedelta64(1, 'h')\n",
    "dfIpTemp['Bins'] = \"\"\n",
    "Bins = []\n",
    "for time in dfIpTemp['timeFromFirstTweet']:\n",
    "    if time <= 6:\n",
    "        Bins.append('0 - 6')\n",
    "    elif time > 6 and time <= 12:\n",
    "        Bins.append('6 - 12')\n",
    "    elif time > 12 and time <= 18:\n",
    "        Bins.append('12 - 18')\n",
    "    elif time > 18 and time <= 24:\n",
    "        Bins.append('18 - 24')\n",
    "    else:\n",
    "        Bins.append('Greater than 24')\n",
    "dfIpTemp['Bins'] = Bins\n",
    "dfIpTemp.loc[dfIpTemp['Bins'] == '0 - 6', '0 - 6 hour bin'] = 1\n",
    "dfIpTemp['0 - 6 hour bin'] = dfIpTemp['0 - 6 hour bin'].fillna(0)\n",
    "#\n",
    "dfIpTemp.loc[dfIpTemp['Bins'] == '6 - 12', '6 - 12 hour bin'] = 1\n",
    "dfIpTemp['6 - 12 hour bin'] = dfIpTemp['6 - 12 hour bin'].fillna(0)\n",
    "#\n",
    "dfIpTemp.loc[dfIpTemp['Bins'] == '12 - 18', '12 - 18 hour bin'] = 1\n",
    "dfIpTemp['12 - 18 hour bin'] = dfIpTemp['12 - 18 hour bin'].fillna(0)\n",
    "#\n",
    "dfIpTemp.loc[dfIpTemp['Bins'] == '18 - 24', '18 - 24 hour bin'] = 1\n",
    "dfIpTemp['18 - 24 hour bin'] = dfIpTemp['18 - 24 hour bin'].fillna(0)\n",
    "#\n",
    "dfIpTemp.loc[dfIpTemp['Bins'] == 'Greater than 24', 'Greater than 24'] = 1\n",
    "dfIpTemp['Greater than 24'] = dfIpTemp['Greater than 24'].fillna(0)\n",
    "#\n",
    "dfIpTemp['count0to6'] = dfIpTemp.groupby('search')['0 - 6 hour bin'].transform('sum')\n",
    "dfIpTemp['count6to12'] = dfIpTemp.groupby('search')['6 - 12 hour bin'].transform('sum')\n",
    "dfIpTemp['count12to18'] = dfIpTemp.groupby('search')['12 - 18 hour bin'].transform('sum')\n",
    "dfIpTemp['count18to24'] = dfIpTemp.groupby('search')['18 - 24 hour bin'].transform('sum')\n",
    "dfIpTemp['count24plus'] = dfIpTemp.groupby('search')['Greater than 24'].transform('sum')\n",
    "#\n",
    "dfIpTemp = dfIpTemp.reset_index(drop=True)\n",
    "dfIpTemp = dfIpTemp.drop_duplicates(subset=['search'], keep='first')\n",
    "dfIpTemp = dfIpTemp.reset_index(drop=True)\n",
    "#\n",
    "dfOut['timeAbsBin1count0to6'] = dfIpTemp['count0to6']\n",
    "dfOut['timeAbsBin2count6to12'] = dfIpTemp['count6to12']\n",
    "dfOut['timeAbsBin3count12to18'] = dfIpTemp['count12to18']\n",
    "dfOut['timeAbsBin4count18to24'] = dfIpTemp['count18to24']\n",
    "dfOut['timeAbsBin5count24plus'] = dfIpTemp['count24plus']\n",
    "#\n",
    "del(dfIpTemp)\n",
    "\n",
    "####################################################\n",
    "## avg Tweets Per Unique User\n",
    "dfIpTemp = dfIpData[['date', 'user_id_str', 'search', 'dateNew']].copy()\n",
    "dfIpTemp['user_id_str'] = dfIpTemp['user_id_str'].astype(str)\n",
    "dfIpTemp['search_user_id_str'] = dfIpTemp['search'] + '___' + dfIpTemp['user_id_str']\n",
    "dfIpTemp['countTweetsForUrl'] = dfIpTemp['search'].map(dfIpTemp['search'].value_counts())\n",
    "dfIpTemp['countTweetsPerUser'] = dfIpTemp['search_user_id_str'].map(dfIpTemp['search_user_id_str'].value_counts())\n",
    "dfIpTemp = dfIpTemp.drop_duplicates(subset = ['search_user_id_str'], keep = 'first')\n",
    "dfIpTemp['countUniqueUsersPerUrl'] = dfIpTemp['search'].map(dfIpTemp['search'].value_counts())\n",
    "del dfIpTemp['search_user_id_str']\n",
    "dfIpTemp['avgTweetsPerUniqUser'] = dfIpTemp['countTweetsForUrl'] / dfIpTemp['countUniqueUsersPerUrl']\n",
    "dfIpTemp = dfIpTemp.drop_duplicates(subset = ['search'], keep = 'first')\n",
    "dfIpTemp = dfIpTemp.reset_index(drop=True)\n",
    "dfOut['avgTweetsPerUniqUser'] = dfIpTemp['avgTweetsPerUniqUser']\n",
    "#\n",
    "del(dfIpTemp)\n",
    "\n",
    "####################################################\n",
    "####################################################\n",
    "## normalize the features\n",
    "scaler = MinMaxScaler()\n",
    "dfOutTempNorm = dfOut[['totalTweets', 'totalRetweets', 'totalLikes', 'totalReplies',           \\\n",
    "                    'countWeekDay', 'countWeekEnd', 'countMonday', 'countTuesday',             \\\n",
    "                    'countWednesday', 'countThursday', 'countFriday', 'countSaturday',         \\\n",
    "                    'countSunday', 'avgTweetsPerUniqUser']]\n",
    "dfOutTempNorm[['totalTweets', 'totalRetweets', 'totalLikes', 'totalReplies',                     \\\n",
    "                    'countWeekDay', 'countWeekEnd', 'countMonday', 'countTuesday',         \\\n",
    "                    'countWednesday', 'countThursday', 'countFriday', 'countSaturday',             \\\n",
    "                    'countSunday', 'avgTweetsPerUniqUser']]   \\\n",
    "= \\\n",
    "scaler.fit_transform(dfOutTempNorm[['totalTweets', 'totalRetweets', 'totalLikes', 'totalReplies', \\\n",
    "                    'countWeekDay', 'countWeekEnd', 'countMonday', 'countTuesday',  \\\n",
    "                    'countWednesday', 'countThursday', 'countFriday', 'countSaturday',      \\\n",
    "                    'countSunday', 'avgTweetsPerUniqUser']])\n",
    "#\n",
    "dfOut['normTotalTweets'] = dfOutTempNorm['totalTweets']\n",
    "dfOut['normTotalRetweets'] = dfOutTempNorm['totalRetweets']\n",
    "dfOut['normTotalLikes'] = dfOutTempNorm['totalLikes']\n",
    "dfOut['normTotalReplies'] = dfOutTempNorm['totalReplies']\n",
    "dfOut['normCountWeekDay'] = dfOutTempNorm['countWeekDay']\n",
    "dfOut['normCountWeekEnd'] = dfOutTempNorm['countWeekEnd']\n",
    "dfOut['normCountMonday'] = dfOutTempNorm['countMonday']\n",
    "dfOut['normCountTuesday'] = dfOutTempNorm['countTuesday']\n",
    "dfOut['normCountWednesday'] = dfOutTempNorm['countWednesday']\n",
    "dfOut['normCountThursday'] = dfOutTempNorm['countThursday']\n",
    "dfOut['normCountFriday'] = dfOutTempNorm['countFriday']\n",
    "dfOut['normCountSaturday'] = dfOutTempNorm['countSaturday']\n",
    "dfOut['normCountSunday'] = dfOutTempNorm['countSunday']\n",
    "dfOut['normAvgTweetsPerUniqUser'] = dfOutTempNorm['avgTweetsPerUniqUser']\n",
    "#\n",
    "del(dfOutTempNorm)\n",
    "#\n",
    "dfBinNormTemp = dfOut[['timeAbsBin1count0to6', 'timeAbsBin2count6to12',     \\\n",
    "                       'timeAbsBin3count12to18', 'timeAbsBin4count18to24',  \\\n",
    "                        'timeAbsBin5count24plus']]\n",
    "#\n",
    "dfBinNormTemp['totalTimeAbsBin5count']   =      \\\n",
    "dfBinNormTemp['timeAbsBin1count0to6']    +      \\\n",
    "dfBinNormTemp['timeAbsBin2count6to12']   +      \\\n",
    "dfBinNormTemp['timeAbsBin3count12to18']  +      \\\n",
    "dfBinNormTemp['timeAbsBin4count18to24']  +      \\\n",
    "dfBinNormTemp['timeAbsBin5count24plus']\n",
    "#\n",
    "dfBinNormTemp['NORMtimeAbsBin1count0to6']   = dfBinNormTemp['timeAbsBin1count0to6']    / dfBinNormTemp['totalTimeAbsBin5count']\n",
    "dfBinNormTemp['NORMtimeAbsBin1count6to12']  = dfBinNormTemp['timeAbsBin2count6to12']   / dfBinNormTemp['totalTimeAbsBin5count']\n",
    "dfBinNormTemp['NORMtimeAbsBin1count12to18'] = dfBinNormTemp['timeAbsBin3count12to18']  / dfBinNormTemp['totalTimeAbsBin5count']\n",
    "dfBinNormTemp['NORMtimeAbsBin1count18to24'] = dfBinNormTemp['timeAbsBin4count18to24']  / dfBinNormTemp['totalTimeAbsBin5count']\n",
    "dfBinNormTemp['NORMtimeAbsBin5count24plus'] = dfBinNormTemp['timeAbsBin5count24plus']  / dfBinNormTemp['totalTimeAbsBin5count']\n",
    "#\n",
    "dfOut['normTimeAbsBin1count0to6']   = dfBinNormTemp['NORMtimeAbsBin1count0to6']\n",
    "dfOut['normTimeAbsBin2count6to12']  = dfBinNormTemp['NORMtimeAbsBin1count6to12']\n",
    "dfOut['normTimeAbsBin3count12to18'] = dfBinNormTemp['NORMtimeAbsBin1count12to18']\n",
    "dfOut['normTimeAbsBin4count18to24'] = dfBinNormTemp['NORMtimeAbsBin1count18to24']\n",
    "dfOut['normTimeAbsBin5count24plus'] = dfBinNormTemp['NORMtimeAbsBin5count24plus']\n",
    "#\n",
    "del(dfBinNormTemp)\n",
    "#\n",
    "## normalise the span, avgTimeBetweenTweets, avgTimeOfNextTweet by first setting the -1 values to 0,\n",
    "#        applying min-max, then reinserting the -1 at the same positions as before\n",
    "dfOutTemp = dfOut[['span', 'avgTimeBetweenTweets', 'avgTimeOfNextTweet']]\n",
    "dfOutTemp['normSpan']                 = dfOutTemp['span']\n",
    "dfOutTemp['normAvgTimeBetweenTweets'] = dfOutTemp['avgTimeBetweenTweets']\n",
    "dfOutTemp['normAvgTimeOfNextTweet']   = dfOutTemp['avgTimeOfNextTweet']\n",
    "dfOutTemp['normSpan']                 = dfOutTemp['normSpan'].replace(-1, 0)\n",
    "dfOutTemp['normAvgTimeBetweenTweets'] = dfOutTemp['normAvgTimeBetweenTweets'].replace(-1, 0)\n",
    "dfOutTemp['normAvgTimeOfNextTweet']   = dfOutTemp['normAvgTimeOfNextTweet'].replace(-1, 0)\n",
    "dfOutTemp[['normSpan', 'normAvgTimeBetweenTweets', 'normAvgTimeOfNextTweet']] =            \\\n",
    "scaler.fit_transform(dfOutTemp[['normSpan', 'normAvgTimeBetweenTweets', 'normAvgTimeOfNextTweet']])\n",
    "dfOutTemp.loc[dfOutTemp['span'] == -1 , 'normSpan'] = -1\n",
    "dfOutTemp.loc[dfOutTemp['avgTimeBetweenTweets'] == -1 , 'normAvgTimeBetweenTweets'] = -1\n",
    "dfOutTemp.loc[dfOutTemp['avgTimeOfNextTweet'] == -1 , 'normAvgTimeOfNextTweet'] = -1\n",
    "dfOut['normSpan']                 = dfOutTemp['normSpan']\n",
    "dfOut['normAvgTimeBetweenTweets'] = dfOutTemp['normAvgTimeBetweenTweets']\n",
    "dfOut['normAvgTimeOfNextTweet']   = dfOutTemp['normAvgTimeOfNextTweet']\n",
    "#\n",
    "del(dfOutTemp)\n",
    "dfOut.rename(columns = {'search':'url'}, inplace = True)\n",
    "#\n",
    "## creating these additional fields which can be used for some data visualisation later\n",
    "dfOut['normPercentWeekDayByTotalTweets']   = dfOut['countWeekDay']   / dfOut['totalTweets']\n",
    "dfOut['normPercentWeekEndByTotalTweets']   = dfOut['countWeekEnd']   / dfOut['totalTweets']\n",
    "dfOut['normPercentMondayByTotalTweets']    = dfOut['countMonday']    / dfOut['totalTweets']\n",
    "dfOut['normPercentTuesdayByTotalTweets']   = dfOut['countTuesday']   / dfOut['totalTweets']\n",
    "dfOut['normPercentWednesdayByTotalTweets'] = dfOut['countWednesday'] / dfOut['totalTweets']\n",
    "dfOut['normPercentThursdayByTotalTweets']  = dfOut['countThursday']  / dfOut['totalTweets']\n",
    "dfOut['normPercentFridayByTotalTweets']    = dfOut['countFriday']    / dfOut['totalTweets']\n",
    "dfOut['normPercentSaturdayByTotalTweets']  = dfOut['countSaturday']  / dfOut['totalTweets']\n",
    "dfOut['normPercentSundayByTotalTweets']    = dfOut['countSunday']    / dfOut['totalTweets']\n",
    "#\n",
    "# deleting the urlType as it is garbage values\n",
    "del dfOut['urlType']\n",
    "del dfOut['domain']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# writing the twitter features file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "opFileDfOutWITHNormValue = '/home/rohit/SRH/CaseStudy1/dataDemo/dataFiles/temp_T_features.csv'\n",
    "dfOut.to_csv(opFileDfOutWITHNormValue, index=False)\n",
    "#\n",
    "del dfOut\n",
    "del dfIpData\n",
    "#\n",
    "### finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M features -- extract MOPHOLOGICAL features (using spaCy) -- using the title+content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipDataFile = urlsFile4Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipInitialRows = 0\n",
    "data = pd.read_csv(ipDataFile, skiprows = range(1, skipInitialRows + 1), sep = ',', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "####################################################\n",
    "## define the functions to be used\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    stripped_text = soup.get_text()\n",
    "    return stripped_text\n",
    "\n",
    "#strip_html_tags('<html><h2>Some important text</h2></html>')\n",
    "#\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "#remove_accented_chars('Sómě Áccěntěd těxt')\n",
    "#\n",
    "def expand_contractions(text, contractions_dict):\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contractions_dict.keys())),\n",
    "                                      flags=re.IGNORECASE | re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        \n",
    "        expanded_contraction = contractions_dict.get(match) \\\n",
    "            if contractions_dict.get(match) \\\n",
    "            else contractions_dict.get(match.lower())\n",
    "        expanded_contraction = expanded_contraction\n",
    "        \n",
    "        return expanded_contraction\n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    expanded_text = \"%s%s\" % (expanded_text[0].upper(), expanded_text[1:])\n",
    "\n",
    "    return expanded_text\n",
    "\n",
    "#expand_contractions(\"You all can't expand contractions I'd think\", contractions_dict)\n",
    "#\n",
    "def remove_special_characters(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text\n",
    "\n",
    "#remove_special_characters(\"Well this was fun! What do you think? 123#@!\", remove_digits=False)\n",
    "#\n",
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "#lemmatize_text(\"My system keeps crashing! his crashed yesterday, ours crashes daily\")\n",
    "#\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "\n",
    "#remove_stopwords(\"The, and, if are stopwords, computer is not\")\n",
    "#\n",
    "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n",
    "                     accented_char_removal=True, text_lower_case=False, \n",
    "                     text_lemmatization=False, special_char_removal=False, \n",
    "                     stopword_removal=True, remove_digits=True):\n",
    "    \n",
    "    normalized_corpus = []\n",
    "    # normalize each document in the corpus\n",
    "    for doc in corpus:\n",
    "        # strip HTML\n",
    "        if html_stripping:\n",
    "            doc = strip_html_tags(doc)\n",
    "        # remove accented characters\n",
    "        if accented_char_removal:\n",
    "            doc = remove_accented_chars(doc)\n",
    "        # expand contractions    \n",
    "        if contraction_expansion:\n",
    "            doc = expand_contractions(doc, contractions_dict)\n",
    "        # lowercase the text    \n",
    "        if text_lower_case:\n",
    "            doc = doc.lower()\n",
    "        # remove extra newlines\n",
    "        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
    "        # lemmatize text\n",
    "        if text_lemmatization:\n",
    "            doc = lemmatize_text(doc)\n",
    "        # remove special characters and\\or digits    \n",
    "        if special_char_removal:\n",
    "            # insert spaces between special characters to isolate them    \n",
    "            special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "            doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "            doc = remove_special_characters(doc, remove_digits=remove_digits)  \n",
    "        # remove extra whitespace\n",
    "        doc = re.sub(' +', ' ', doc)\n",
    "        # remove stopwords\n",
    "        if stopword_removal:\n",
    "            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
    "            \n",
    "        normalized_corpus.append(doc)\n",
    "        \n",
    "    return normalized_corpus\n",
    "#\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')\n",
    "nlp = en_core_web_lg.load()\n",
    "#\n",
    "data['full_text'] = data[\"title\"].map(str)+ '. ' + data[\"content\"]\n",
    "del data['title']\n",
    "del data['content']\n",
    "#\n",
    "data['clean_full_text'] = normalize_corpus(data['full_text'])\n",
    "del data['full_text']\n",
    "#\n",
    "dfOut = data.copy()\n",
    "## there are 50 features\n",
    "new_cols = ['$', \"''\", ',', '-LRB-', '-RRB-', '.', ':', 'ADD', 'AFX', 'CC', 'CD', 'DT', 'EX', 'FW', 'HYPH', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NFP', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', 'XX', '_SP', '``']\n",
    "dfOut = dfOut.assign(**dict.fromkeys(new_cols, -1.00))\n",
    "## no need to keep the data dataframe anymore\n",
    "del(data)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished row number = 0\n",
      "for url = http://beforeitsnews.com/blogging-citizen-journalism/2018/01/attention-forward-to-president-trump-bombshell-breadcrumb-from-judge-anna-von-reitz-urgent-public-message-2585490.html\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "printFreqValue = 1\n",
    "# POS tagging for sample cleaned full text\n",
    "l = 0\n",
    "#mf_clean_full_text = data3\n",
    "while l < len(dfOut):\n",
    "    sentence = str(dfOut.iloc[l].clean_full_text)\n",
    "    sentence_nlp = nlp(sentence)\n",
    "    # POS tagging with Spacy \n",
    "    spacy_pos_tagged = [(word, word.tag_, word.pos_) for word in sentence_nlp]\n",
    "    postag_data = pd.DataFrame(spacy_pos_tagged, columns=['Word', 'POS tag', 'Tag type'])\n",
    "    mb = postag_data.groupby(['POS tag']).count()\n",
    "    mc = mb.drop(['Tag type'], axis = 1)\n",
    "    mc = mc.rename_axis(\"\", axis=\"rows\").transpose()\n",
    "    mc.index = [l]\n",
    "    dfOut.update(mc)\n",
    "    l=l+1\n",
    "    if l % printFreqValue == 0:\n",
    "        print(f\"Finished row number = {l - 1}\\nfor url = {dfOut.iloc[l-1].url}\")\n",
    "#\n",
    "del dfOut['clean_full_text']\n",
    "print(f\"\\nDone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# renaming 6 columns to more meaningful names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "######     The six columns, old and new names\n",
    "#\n",
    "## $       dollar\n",
    "## ''      dblApostrophe\n",
    "## ,       dblApostrophe\n",
    "## .       dot\n",
    "## :       colon\n",
    "## ``      dblSpecialApostrophe\n",
    "#\n",
    "dfOut.rename(columns = { '$'  : 'dollar'},               inplace = True)\n",
    "dfOut.rename(columns = { \"''\" : 'dblApostrophe'},        inplace = True)\n",
    "dfOut.rename(columns = { ','  : 'comma'},                inplace = True)\n",
    "dfOut.rename(columns = { '.'  : 'dot'},                  inplace = True)\n",
    "dfOut.rename(columns = { ':'  : 'colon'},                inplace = True)\n",
    "dfOut.rename(columns = { '``' : 'dblSpecialApostrophe'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "####################################################\n",
    "## normalize the features\n",
    "scaler = MinMaxScaler()\n",
    "dfOutTempNorm = dfOut.copy()\n",
    "del dfOutTempNorm['domain']\n",
    "del dfOutTempNorm['url']\n",
    "#\n",
    "dfOutTempNorm['normDollar']               = dfOutTempNorm['dollar']\n",
    "dfOutTempNorm['normDblApostrophe']        = dfOutTempNorm['dblApostrophe']\n",
    "dfOutTempNorm['normComma']                = dfOutTempNorm['comma']\n",
    "dfOutTempNorm['norm-LRB-']                = dfOutTempNorm['-LRB-']\n",
    "dfOutTempNorm['norm-RRB-']                = dfOutTempNorm['-RRB-']\n",
    "dfOutTempNorm['normDot']                  = dfOutTempNorm['dot']\n",
    "dfOutTempNorm['normColon']                = dfOutTempNorm['colon']\n",
    "dfOutTempNorm['normADD']                  = dfOutTempNorm['ADD']\n",
    "dfOutTempNorm['normAFX']                  = dfOutTempNorm['AFX']\n",
    "dfOutTempNorm['normCC']                   = dfOutTempNorm['CC']\n",
    "dfOutTempNorm['normCD']                   = dfOutTempNorm['CD']\n",
    "dfOutTempNorm['normDT']                   = dfOutTempNorm['DT']\n",
    "dfOutTempNorm['normEX']                   = dfOutTempNorm['EX']\n",
    "dfOutTempNorm['normFW']                   = dfOutTempNorm['FW']\n",
    "dfOutTempNorm['normHYPH']                 = dfOutTempNorm['HYPH']\n",
    "dfOutTempNorm['normIN']                   = dfOutTempNorm['IN']\n",
    "dfOutTempNorm['normJJ']                   = dfOutTempNorm['JJ']\n",
    "dfOutTempNorm['normJJR']                  = dfOutTempNorm['JJR']\n",
    "dfOutTempNorm['normJJS']                  = dfOutTempNorm['JJS']\n",
    "dfOutTempNorm['normLS']                   = dfOutTempNorm['LS']\n",
    "dfOutTempNorm['normMD']                   = dfOutTempNorm['MD']\n",
    "dfOutTempNorm['normNFP']                  = dfOutTempNorm['NFP']\n",
    "dfOutTempNorm['normNN']                   = dfOutTempNorm['NN']\n",
    "dfOutTempNorm['normNNP']                  = dfOutTempNorm['NNP']\n",
    "dfOutTempNorm['normNNPS']                 = dfOutTempNorm['NNPS']\n",
    "dfOutTempNorm['normNNS']                  = dfOutTempNorm['NNS']\n",
    "dfOutTempNorm['normPDT']                  = dfOutTempNorm['PDT']\n",
    "dfOutTempNorm['normPOS']                  = dfOutTempNorm['POS']\n",
    "dfOutTempNorm['normPRP']                  = dfOutTempNorm['PRP']\n",
    "dfOutTempNorm['normPRP$']                 = dfOutTempNorm['PRP$']\n",
    "dfOutTempNorm['normRB']                   = dfOutTempNorm['RB']\n",
    "dfOutTempNorm['normRBR']                  = dfOutTempNorm['RBR']\n",
    "dfOutTempNorm['normRBS']                  = dfOutTempNorm['RBS']\n",
    "dfOutTempNorm['normRP']                   = dfOutTempNorm['RP']\n",
    "dfOutTempNorm['normSYM']                  = dfOutTempNorm['SYM']\n",
    "dfOutTempNorm['normTO']                   = dfOutTempNorm['TO']\n",
    "dfOutTempNorm['normUH']                   = dfOutTempNorm['UH']\n",
    "dfOutTempNorm['normVB']                   = dfOutTempNorm['VB']\n",
    "dfOutTempNorm['normVBD']                  = dfOutTempNorm['VBD']\n",
    "dfOutTempNorm['normVBG']                  = dfOutTempNorm['VBG']\n",
    "dfOutTempNorm['normVBN']                  = dfOutTempNorm['VBN']\n",
    "dfOutTempNorm['normVBP']                  = dfOutTempNorm['VBP']\n",
    "dfOutTempNorm['normVBZ']                  = dfOutTempNorm['VBZ']\n",
    "dfOutTempNorm['normWDT']                  = dfOutTempNorm['WDT']\n",
    "dfOutTempNorm['normWP']                   = dfOutTempNorm['WP']\n",
    "dfOutTempNorm['normWP$']                  = dfOutTempNorm['WP$']\n",
    "dfOutTempNorm['normWRB']                  = dfOutTempNorm['WRB']\n",
    "dfOutTempNorm['normXX']                   = dfOutTempNorm['XX']\n",
    "dfOutTempNorm['norm_SP']                  = dfOutTempNorm['_SP']\n",
    "dfOutTempNorm['normDblSpecialApostrophe'] = dfOutTempNorm['dblSpecialApostrophe']\n",
    "#\n",
    "dfOutTempNorm['normDollar']               = dfOutTempNorm['normDollar'].replace(-1, 0)\n",
    "dfOutTempNorm['normDblApostrophe']        = dfOutTempNorm['normDblApostrophe'].replace(-1, 0)\n",
    "dfOutTempNorm['normComma']                = dfOutTempNorm['normComma'].replace(-1, 0)\n",
    "dfOutTempNorm['norm-LRB-']                = dfOutTempNorm['norm-LRB-'].replace(-1, 0)\n",
    "dfOutTempNorm['norm-RRB-']                = dfOutTempNorm['norm-RRB-'].replace(-1, 0)\n",
    "dfOutTempNorm['normDot']                  = dfOutTempNorm['normDot'].replace(-1, 0)\n",
    "dfOutTempNorm['normColon']                = dfOutTempNorm['normColon'].replace(-1, 0)\n",
    "dfOutTempNorm['normADD']                  = dfOutTempNorm['normADD'].replace(-1, 0)\n",
    "dfOutTempNorm['normAFX']                  = dfOutTempNorm['normAFX'].replace(-1, 0)\n",
    "dfOutTempNorm['normCC']                   = dfOutTempNorm['normCC'].replace(-1, 0)\n",
    "dfOutTempNorm['normCD']                   = dfOutTempNorm['normCD'].replace(-1, 0)\n",
    "dfOutTempNorm['normDT']                   = dfOutTempNorm['normDT'].replace(-1, 0)\n",
    "dfOutTempNorm['normEX']                   = dfOutTempNorm['normEX'].replace(-1, 0)\n",
    "dfOutTempNorm['normFW']                   = dfOutTempNorm['normFW'].replace(-1, 0)\n",
    "dfOutTempNorm['normHYPH']                 = dfOutTempNorm['normHYPH'].replace(-1, 0)\n",
    "dfOutTempNorm['normIN']                   = dfOutTempNorm['normIN'].replace(-1, 0)\n",
    "dfOutTempNorm['normJJ']                   = dfOutTempNorm['normJJ'].replace(-1, 0)\n",
    "dfOutTempNorm['normJJR']                  = dfOutTempNorm['normJJR'].replace(-1, 0)\n",
    "dfOutTempNorm['normJJS']                  = dfOutTempNorm['normJJS'].replace(-1, 0)\n",
    "dfOutTempNorm['normLS']                   = dfOutTempNorm['normLS'].replace(-1, 0)\n",
    "dfOutTempNorm['normMD']                   = dfOutTempNorm['normMD'].replace(-1, 0)\n",
    "dfOutTempNorm['normNFP']                  = dfOutTempNorm['normNFP'].replace(-1, 0)\n",
    "dfOutTempNorm['normNN']                   = dfOutTempNorm['normNN'].replace(-1, 0)\n",
    "dfOutTempNorm['normNNPS']                 = dfOutTempNorm['normNNPS'].replace(-1, 0)\n",
    "dfOutTempNorm['normNNS']                  = dfOutTempNorm['normNNS'].replace(-1, 0)\n",
    "dfOutTempNorm['normPDT']                  = dfOutTempNorm['normPDT'].replace(-1, 0)\n",
    "dfOutTempNorm['normPOS']                  = dfOutTempNorm['normPOS'].replace(-1, 0)\n",
    "dfOutTempNorm['normPRP']                  = dfOutTempNorm['normPRP'].replace(-1, 0)\n",
    "dfOutTempNorm['normPRP$']                 = dfOutTempNorm['normPRP$'].replace(-1, 0)\n",
    "dfOutTempNorm['normRB']                   = dfOutTempNorm['normRB'].replace(-1, 0)\n",
    "dfOutTempNorm['normRBR']                  = dfOutTempNorm['normRBR'].replace(-1, 0)\n",
    "dfOutTempNorm['normRBS']                  = dfOutTempNorm['normRBS'].replace(-1, 0)\n",
    "dfOutTempNorm['normRP']                   = dfOutTempNorm['normRP'].replace(-1, 0)\n",
    "dfOutTempNorm['normSYM']                  = dfOutTempNorm['normSYM'].replace(-1, 0)\n",
    "dfOutTempNorm['normTO']                   = dfOutTempNorm['normTO'].replace(-1, 0)\n",
    "dfOutTempNorm['normUH']                   = dfOutTempNorm['normUH'].replace(-1, 0)\n",
    "dfOutTempNorm['normVB']                   = dfOutTempNorm['normVB'].replace(-1, 0)\n",
    "dfOutTempNorm['normVBD']                  = dfOutTempNorm['normVBD'].replace(-1, 0)\n",
    "dfOutTempNorm['normVBG']                  = dfOutTempNorm['normVBG'].replace(-1, 0)\n",
    "dfOutTempNorm['normVBN']                  = dfOutTempNorm['normVBN'].replace(-1, 0)\n",
    "dfOutTempNorm['normVBP']                  = dfOutTempNorm['normVBP'].replace(-1, 0)\n",
    "dfOutTempNorm['normVBZ']                  = dfOutTempNorm['normVBZ'].replace(-1, 0)\n",
    "dfOutTempNorm['normWDT']                  = dfOutTempNorm['normWDT'].replace(-1, 0)\n",
    "dfOutTempNorm['normWP']                   = dfOutTempNorm['normWP'].replace(-1, 0)\n",
    "dfOutTempNorm['normWP$']                  = dfOutTempNorm['normWP$'].replace(-1, 0)\n",
    "dfOutTempNorm['normWRB']                  = dfOutTempNorm['normWRB'].replace(-1, 0)\n",
    "dfOutTempNorm['normXX']                   = dfOutTempNorm['normXX'].replace(-1, 0)\n",
    "dfOutTempNorm['norm_SP']                  = dfOutTempNorm['norm_SP'].replace(-1, 0)\n",
    "dfOutTempNorm['normDblSpecialApostrophe'] = dfOutTempNorm['normDblSpecialApostrophe'].replace(-1, 0)\n",
    "#\n",
    "dfOutTempNorm[['normDollar', 'normDblApostrophe', 'normComma', 'norm-LRB-',\n",
    " 'norm-RRB-', 'normDot', 'normColon', 'normADD', 'normAFX', 'normCC', 'normCD',\n",
    " 'normDT', 'normEX', 'normFW', 'normHYPH', 'normIN', 'normJJ', 'normJJR', 'normJJS',\n",
    " 'normLS', 'normMD', 'normNFP', 'normNN', 'normNNP', 'normNNPS', 'normNNS', 'normPDT',\n",
    " 'normPOS', 'normPRP', 'normPRP$', 'normRB', 'normRBR', 'normRBS', 'normRP',\n",
    " 'normSYM', 'normTO', 'normUH', 'normVB', 'normVBD', 'normVBG', 'normVBN',\n",
    " 'normVBP', 'normVBZ', 'normWDT', 'normWP', 'normWP$', 'normWRB',\n",
    " 'normXX', 'norm_SP', 'normDblSpecialApostrophe']] = \\\n",
    "scaler.fit_transform(dfOutTempNorm[['normDollar', 'normDblApostrophe', 'normComma', 'norm-LRB-',\n",
    " 'norm-RRB-', 'normDot', 'normColon', 'normADD', 'normAFX', 'normCC', 'normCD',\n",
    " 'normDT', 'normEX', 'normFW', 'normHYPH', 'normIN', 'normJJ', 'normJJR', 'normJJS',\n",
    " 'normLS', 'normMD', 'normNFP', 'normNN', 'normNNP', 'normNNPS', 'normNNS', 'normPDT',\n",
    " 'normPOS', 'normPRP', 'normPRP$', 'normRB', 'normRBR', 'normRBS', 'normRP',\n",
    " 'normSYM', 'normTO', 'normUH', 'normVB', 'normVBD', 'normVBG', 'normVBN',\n",
    " 'normVBP', 'normVBZ', 'normWDT', 'normWP', 'normWP$', 'normWRB',\n",
    " 'normXX', 'norm_SP', 'normDblSpecialApostrophe']])\n",
    "#\n",
    "dfOutTempNorm.loc[dfOutTempNorm['dollar']               == -1 , 'normDollar'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['dblApostrophe']        == -1 , 'normDblApostrophe'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['comma']                == -1 , 'normComma'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['-LRB-']                == -1 , 'norm-LRB-'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['-RRB-']                == -1 , 'norm-RRB-'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['dot']                  == -1 , 'normDot'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['colon']                == -1 , 'normColon'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['ADD']                  == -1 , 'normADD'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['AFX']                  == -1 , 'normAFX'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['CC']                   == -1 , 'normCC'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['CD']                   == -1 , 'normCD'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['DT']                   == -1 , 'normDT'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['EX']                   == -1 , 'normEX'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['FW']                   == -1 , 'normFW'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['HYPH']                 == -1 , 'normHYPH'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['IN']                   == -1 , 'normIN'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['JJ']                   == -1 , 'normJJ'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['JJR']                  == -1 , 'normJJR'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['JJS']                  == -1 , 'normJJS'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['LS']                   == -1 , 'normLS'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['MD']                   == -1 , 'normMD'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['NFP']                  == -1 , 'normNFP'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['NN']                   == -1 , 'normNN'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['NNPS']                 == -1 , 'normNNPS'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['NNS']                  == -1 , 'normNNS'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['PDT']                  == -1 , 'normPDT'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['POS']                  == -1 , 'normPOS'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['PRP']                  == -1 , 'normPRP'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['PRP$']                 == -1 , 'normPRP$'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['RB']                   == -1 , 'normRB'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['RBR']                  == -1 , 'normRBR'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['RBS']                  == -1 , 'normRBS'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['RP']                   == -1 , 'normRP'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['SYM']                  == -1 , 'normSYM'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['TO']                   == -1 , 'normTO'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['UH']                   == -1 , 'normUH'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['VB']                   == -1 , 'normVB'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['VBD']                  == -1 , 'normVBD'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['VBG']                  == -1 , 'normVBG'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['VBN']                  == -1 , 'normVBN'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['VBP']                  == -1 , 'normVBP'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['VBZ']                  == -1 , 'normVBZ'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['WDT']                  == -1 , 'normWDT'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['WP']                   == -1 , 'normWP'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['WP$']                  == -1 , 'normWP$'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['WRB']                  == -1 , 'normWRB'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['XX']                   == -1 , 'normXX'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['_SP']                  == -1 , 'norm_SP'] = -1\n",
    "dfOutTempNorm.loc[dfOutTempNorm['dblSpecialApostrophe'] == -1 , 'normDblSpecialApostrophe'] = -1\n",
    "#\n",
    "dfOut['normDollar']               = dfOutTempNorm['normDollar']\n",
    "dfOut['normDblApostrophe']        = dfOutTempNorm['normDblApostrophe']\n",
    "dfOut['normComma']                = dfOutTempNorm['normComma']\n",
    "dfOut['norm-LRB-']                = dfOutTempNorm['norm-LRB-']\n",
    "dfOut['norm-RRB-']                = dfOutTempNorm['norm-RRB-']\n",
    "dfOut['normDot']                  = dfOutTempNorm['normDot']\n",
    "dfOut['normColon']                = dfOutTempNorm['normColon']\n",
    "dfOut['normADD']                  = dfOutTempNorm['normADD']\n",
    "dfOut['normAFX']                  = dfOutTempNorm['normAFX']\n",
    "dfOut['normCC']                   = dfOutTempNorm['normCC']\n",
    "dfOut['normCD']                   = dfOutTempNorm['normCD']\n",
    "dfOut['normDT']                   = dfOutTempNorm['normDT']\n",
    "dfOut['normEX']                   = dfOutTempNorm['normEX']\n",
    "dfOut['normFW']                   = dfOutTempNorm['normFW']\n",
    "dfOut['normHYPH']                 = dfOutTempNorm['normHYPH']\n",
    "dfOut['normIN']                   = dfOutTempNorm['normIN']\n",
    "dfOut['normJJ']                   = dfOutTempNorm['normJJ']\n",
    "dfOut['normJJR']                  = dfOutTempNorm['normJJR']\n",
    "dfOut['normJJS']                  = dfOutTempNorm['normJJS']\n",
    "dfOut['normLS']                   = dfOutTempNorm['normLS']\n",
    "dfOut['normMD']                   = dfOutTempNorm['normMD']\n",
    "dfOut['normNFP']                  = dfOutTempNorm['normNFP']\n",
    "dfOut['normNN']                   = dfOutTempNorm['normNN']\n",
    "dfOut['normNNP']                  = dfOutTempNorm['normNNP']\n",
    "dfOut['normNNPS']                 = dfOutTempNorm['normNNPS']\n",
    "dfOut['normNNS']                  = dfOutTempNorm['normNNS']\n",
    "dfOut['normPDT']                  = dfOutTempNorm['normPDT']\n",
    "dfOut['normPOS']                  = dfOutTempNorm['normPOS']\n",
    "dfOut['normPRP']                  = dfOutTempNorm['normPRP']\n",
    "dfOut['normPRP$']                 = dfOutTempNorm['normPRP$']\n",
    "dfOut['normRB']                   = dfOutTempNorm['normRB']\n",
    "dfOut['normRBR']                  = dfOutTempNorm['normRBR']\n",
    "dfOut['normRBS']                  = dfOutTempNorm['normRBS']\n",
    "dfOut['normRP']                   = dfOutTempNorm['normRP']\n",
    "dfOut['normSYM']                  = dfOutTempNorm['normSYM']\n",
    "dfOut['normTO']                   = dfOutTempNorm['normTO']\n",
    "dfOut['normUH']                   = dfOutTempNorm['normUH']\n",
    "dfOut['normVB']                   = dfOutTempNorm['normVB']\n",
    "dfOut['normVBD']                  = dfOutTempNorm['normVBD']\n",
    "dfOut['normVBG']                  = dfOutTempNorm['normVBG']\n",
    "dfOut['normVBN']                  = dfOutTempNorm['normVBN']\n",
    "dfOut['normVBP']                  = dfOutTempNorm['normVBP']\n",
    "dfOut['normVBZ']                  = dfOutTempNorm['normVBZ']\n",
    "dfOut['normWDT']                  = dfOutTempNorm['normWDT']\n",
    "dfOut['normWP']                   = dfOutTempNorm['normWP']\n",
    "dfOut['normWP$']                  = dfOutTempNorm['normWP$']\n",
    "dfOut['normWRB']                  = dfOutTempNorm['normWRB']\n",
    "dfOut['normXX']                   = dfOutTempNorm['normXX']\n",
    "dfOut['norm_SP']                  = dfOutTempNorm['norm_SP']\n",
    "dfOut['normDblSpecialApostrophe'] = dfOutTempNorm['normDblSpecialApostrophe']\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## writing the morphological features file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "outFile_WITH_norm = '/home/rohit/SRH/CaseStudy1/dataDemo/dataFiles/temp_M_features.csv'\n",
    "dfOut.to_csv(outFile_WITH_norm, index=False)\n",
    "#\n",
    "del dfOut\n",
    "del dfOutTempNorm\n",
    "#\n",
    "### finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R features -- extract READABILITY features (using Textstat) -- using the title+content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this script to process the datasets for READABILITY FEATURES EXTRACTION.\n",
    "# Run the rogram with EXACTLY 5 argruments as shown below:\n",
    "#       python <script> <input-file-name> <output-CSV-file> <#skipInitialRows> <#rowsToProcess> <#rowsAfterWhichToShowPrintMessageTracker>\n",
    "#\n",
    "#       #skipInitialRows\n",
    "#               is an integer. Is the CSV data row number (0 being for header) from which the data\n",
    "#               should be loaded. Here 0 is the header so to skip first 5 data rows: skipInitialRows = 5.\n",
    "#       #rowsToProcess\n",
    "#               is an integer. If = -1 then will read in all the rows after skipping as per above parameter.\n",
    "#               Otherwise, will read in the number specified.\n",
    "#\n",
    "# The packages expected by the script are:\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import spacy\n",
    "# import nltk\n",
    "# from nltk.tokenize.toktok import ToktokTokenizer\n",
    "# import re\n",
    "# from bs4 import BeautifulSoup\n",
    "# import unicodedata\n",
    "# import en_core_web_lg\n",
    "# import en_core_web_sm\n",
    "# from contractions import contractions_dict\n",
    "# from textstat.textstat import textstatistics, easy_word_set, legacy_round\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# import csv\n",
    "# import sys\n",
    "# from datetime import datetime\n",
    "# import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## python script will run and create the readability features file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note that the print statements about number columns in the dataframe.shape shows a value greater\n",
    "###     by one (4 instead of 3, 23 instead of 22). This is fine. The script was written expecting the\n",
    "###     input data to file to hold only the columns: domain, url, title and content.\n",
    "###     But here was are running with an additional column for urlType and that is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All command line arguments are valid....starting main processing\n",
      "\n",
      "Start time: Tue Oct 22 10:36:32 2019\n",
      "\n",
      "\n",
      "Processing with command line arguments as:\n",
      "1) dataIpFile       = /home/rohit/SRH/CaseStudy1/dataDemo/dataFiles/demorun_input_data.csv\n",
      "2) csvOpFile        = /home/rohit/SRH/CaseStudy1/dataDemo/dataFiles/temp_R_features.csv\n",
      "3) skipInitialRows  = 0\n",
      "4) rowsToProcess    = 15\n",
      "5) printMessageFreq = 1\n",
      "\n",
      "\n",
      "Input data read into dataframe.\n",
      "\n",
      "\n",
      "Normalized the full_text into clean_full_text\n",
      "\n",
      "The shape should be = number of urls, 3 columns\n",
      "data1.shape = (1, 4)\n",
      "The shape should be = number of urls, 22 columns\n",
      "data1.shape = (1, 23)\n",
      "\n",
      "\n",
      "Starting the processing for readability features at:\n",
      "Tue Oct 22 10:36:38 2019\n",
      "\n",
      "\n",
      "\n",
      "Processed url = http://beforeitsnews.com/blogging-citizen-journalism/2018/01/attention-forward-to-president-trump-bombshell-breadcrumb-from-judge-anna-von-reitz-urgent-public-message-2585490.html\n",
      "at row number = 0\n",
      "The shape should be = number of urls, 21 columns, domain url, clean_text and 18 features.\n",
      "data1.shape = (1, 22)\n",
      "\n",
      "\n",
      "Output file written to =\n",
      "/home/rohit/SRH/CaseStudy1/dataDemo/dataFiles/temp_R_features.csv\n",
      "Number of rows skipped   = 0\n",
      "Number of rows processed   = 15\n",
      "\n",
      "\n",
      "End time: Tue Oct 22 10:37:52 2019\n"
     ]
    }
   ],
   "source": [
    "!python /home/rohit/SRH/CaseStudy1/dataDemo/code/scriptCreateReadabilityFeatures1.py /home/rohit/SRH/CaseStudy1/dataDemo/dataFiles/demorun_input_data.csv /home/rohit/SRH/CaseStudy1/dataDemo/dataFiles/temp_R_features.csv 0 15 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L features -- extract PSYCHOLOGICAL features (using LIWC)   -- using the title+content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a paid tool and there is no script for it. You pass it a CSV file with the textual data to be\n",
    "#       analyzed and it returns 50 features.\n",
    "# We are concatenating the Title+Content into so called \"Full_Text\" and using the tool to extract the\n",
    "#       features for each URL.\n",
    "# Thus, after the Psychological features are extracted using LIWC, it should be saved as a file:\n",
    "#       temp_L_features.csv.\n",
    "#\n",
    "# The columns of the file should be as follows:\n",
    "#\n",
    "## So we will use the input file which has the domain, url, title, content.\n",
    "#       Load it into dataframe, combine the title+content as \"full_text\".\n",
    "#       Perform some basic cleaning on the \"full_text\" to create a new column \"clean_full_text\".\n",
    "#       Then this columns data is extracted as a csv file containing only this \"clean_full_text\".\n",
    "#       LIWC tool returns a csv file with columns as \"clean_full_text\" unchanged, followed by\n",
    "#       93 features per row.\n",
    "#\n",
    "# This output file is the \"temp_L_features.csv\" which is used in the workflow.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine the invidual features files for L, M, R and T into one file.\n",
    "\n",
    "## Merging all the individual feature files into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "folderPath = '/home/rohit/SRH/CaseStudy1/dataDemo/dataFiles/'\n",
    "skipInitialRows = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all the features in order of M , L , T , R\n",
    "inFile = folderPath + 'temp_M_features.csv'\n",
    "dfTempM = pd.read_csv(inFile, skiprows = range(1, skipInitialRows + 1), sep = ',', low_memory=False)\n",
    "inFile = folderPath + 'temp_L_features.csv'\n",
    "dfTempL = pd.read_csv(inFile, skiprows = range(1, skipInitialRows + 1), sep = ',', low_memory=False)\n",
    "inFile = folderPath + 'temp_T_features.csv'\n",
    "dfTempT = pd.read_csv(inFile, skiprows = range(1, skipInitialRows + 1), sep = ',', low_memory=False)\n",
    "inFile = folderPath + 'temp_R_features.csv'\n",
    "dfTempR = pd.read_csv(inFile, skiprows = range(1, skipInitialRows + 1), sep = ',', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M features = \n",
      "domain , url , urlType , dollar , dblApostrophe , comma , -LRB- , -RRB- , dot , colon , ADD , AFX , CC , CD , DT , EX , FW , HYPH , IN , JJ , JJR , JJS , LS , MD , NFP , NN , NNP , NNPS , NNS , PDT , POS , PRP , PRP$ , RB , RBR , RBS , RP , SYM , TO , UH , VB , VBD , VBG , VBN , VBP , VBZ , WDT , WP , WP$ , WRB , XX , _SP , dblSpecialApostrophe , normDollar , normDblApostrophe , normComma , norm-LRB- , norm-RRB- , normDot , normColon , normADD , normAFX , normCC , normCD , normDT , normEX , normFW , normHYPH , normIN , normJJ , normJJR , normJJS , normLS , normMD , normNFP , normNN , normNNP , normNNPS , normNNS , normPDT , normPOS , normPRP , normPRP$ , normRB , normRBR , normRBS , normRP , normSYM , normTO , normUH , normVB , normVBD , normVBG , normVBN , normVBP , normVBZ , normWDT , normWP , normWP$ , normWRB , normXX , norm_SP , normDblSpecialApostrophe , \n",
      "\n",
      "L features = \n",
      "domain , url , WC , Analytic , Clout , Authentic , Tone , WPS , Sixltr , Dic , function , pronoun , ppron , i , we , you , shehe , they , ipron , article , prep , auxverb , adverb , conj , negate , verb , adj , compare , interrog , number , quant , affect , posemo , negemo , anx , anger , sad , social , family , friend , female , male , cogproc , insight , cause , discrep , tentat , certain , differ , percept , see , hear , feel , bio , body , health , sexual , ingest , drives , affiliation , achieve , power , reward , risk , focuspast , focuspresent , focusfuture , relativ , motion , space , time , work , leisure , home , money , relig , death , informal , swear , netspeak , assent , nonflu , filler , AllPunc , Period , Comma , Colon , SemiC , QMark , Exclam , Dash , Quote , Apostro , Parenth , OtherP , norm_WC , norm_Analytic , norm_Clout , norm_Authentic , norm_Tone , norm_WPS , norm_Sixltr , norm_Dic , norm_function , norm_pronoun , norm_ppron , norm_i , norm_we , norm_you , norm_shehe , norm_they , norm_ipron , norm_article , norm_prep , norm_auxverb , norm_adverb , norm_conj , norm_negate , norm_verb , norm_adj , norm_compare , norm_interrog , norm_number , norm_quant , norm_affect , norm_posemo , norm_negemo , norm_anx , norm_anger , norm_sad , norm_social , norm_family , norm_friend , norm_female , norm_male , norm_cogproc , norm_insight , norm_cause , norm_discrep , norm_tentat , norm_certain , norm_differ , norm_percept , norm_see , norm_hear , norm_feel , norm_bio , norm_body , norm_health , norm_sexual , norm_ingest , norm_drives , norm_affiliation , norm_achieve , norm_power , norm_reward , norm_risk , norm_focuspast , norm_focuspresent , norm_focusfuture , norm_relativ , norm_motion , norm_space , norm_time , norm_work , norm_leisure , norm_home , norm_money , norm_relig , norm_death , norm_informal , norm_swear , norm_netspeak , norm_assent , norm_nonflu , norm_filler , norm_AllPunc , norm_Period , norm_Comma , norm_Colon , norm_SemiC , norm_QMark , norm_Exclam , norm_Dash , norm_Quote , norm_Apostro , norm_Parenth , norm_OtherP , \n",
      "\n",
      "T features = \n",
      "url , totalTweets , totalRetweets , totalLikes , totalReplies , span , countWeekDay , countWeekEnd , countMonday , countTuesday , countWednesday , countThursday , countFriday , countSaturday , countSunday , avgTimeBetweenTweets , avgTimeOfNextTweet , timeAbsBin1count0to6 , timeAbsBin2count6to12 , timeAbsBin3count12to18 , timeAbsBin4count18to24 , timeAbsBin5count24plus , avgTweetsPerUniqUser , normTotalTweets , normTotalRetweets , normTotalLikes , normTotalReplies , normCountWeekDay , normCountWeekEnd , normCountMonday , normCountTuesday , normCountWednesday , normCountThursday , normCountFriday , normCountSaturday , normCountSunday , normAvgTweetsPerUniqUser , normTimeAbsBin1count0to6 , normTimeAbsBin2count6to12 , normTimeAbsBin3count12to18 , normTimeAbsBin4count18to24 , normTimeAbsBin5count24plus , normSpan , normAvgTimeBetweenTweets , normAvgTimeOfNextTweet , normPercentWeekDayByTotalTweets , normPercentWeekEndByTotalTweets , normPercentMondayByTotalTweets , normPercentTuesdayByTotalTweets , normPercentWednesdayByTotalTweets , normPercentThursdayByTotalTweets , normPercentFridayByTotalTweets , normPercentSaturdayByTotalTweets , normPercentSundayByTotalTweets , \n",
      "\n",
      "R features = \n",
      "domain , url , urlType , full_text_Flesch_reading_ease , full_text_Flesch_kincaid_grade , full_text_smog , full_text_gunning_fog , full_text_words_per_sentence , full_text_capitalized_words , full_text_lexicon , full_text_urls_counts , full_text_long_words , full_text_syllables , full_text_stop_words , full_text_sentences , full_text_linsear_write , full_text_automated_readability , full_text_characters_total , full_text_coleman_liax , full_text_difficult_words , full_text_words_total , norm_full_text_Flesch_reading_ease , norm_full_text_Flesch_kincaid_grade , norm_full_text_smog , norm_full_text_gunning_fog , norm_full_text_words_per_sentence , norm_full_text_capitalized_words , norm_full_text_lexicon , norm_full_text_urls_counts , norm_full_text_long_words , norm_full_text_syllables , norm_full_text_stop_words , norm_full_text_sentences , norm_full_text_linsear_write , norm_full_text_automated_readability , norm_full_text_characters_total , norm_full_text_coleman_liax , norm_full_text_difficult_words , norm_full_text_words_total , \n",
      "\n"
     ]
    }
   ],
   "source": [
    "######## temp code  ###   temp code   ###\n",
    "#\n",
    "# see the columns for each of the files\n",
    "#\n",
    "outColStr = \"\"\n",
    "for col in list(dfTempM) :\n",
    "    outColStr += col + ' , '\n",
    "print(f\"M features = \\n{outColStr}\\n\")\n",
    "#\n",
    "outColStr = \"\"\n",
    "for col in list(dfTempL) :\n",
    "    outColStr += col + ' , '\n",
    "print(f\"L features = \\n{outColStr}\\n\")\n",
    "#\n",
    "outColStr = \"\"\n",
    "for col in list(dfTempT) :\n",
    "    outColStr += col + ' , '\n",
    "print(f\"T features = \\n{outColStr}\\n\")\n",
    "#\n",
    "outColStr = \"\"\n",
    "for col in list(dfTempR) :\n",
    "    outColStr += col + ' , '\n",
    "print(f\"R features = \\n{outColStr}\\n\")\n",
    "#\n",
    "######## temp code  ###   temp code   ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_ALL_Out_WITH_norm features = \n",
      "domain , url , urlType , dollar , dblApostrophe , comma , -LRB- , -RRB- , dot , colon , ADD , AFX , CC , CD , DT , EX , FW , HYPH , IN , JJ , JJR , JJS , LS , MD , NFP , NN , NNP , NNPS , NNS , PDT , POS , PRP , PRP$ , RB , RBR , RBS , RP , SYM , TO , UH , VB , VBD , VBG , VBN , VBP , VBZ , WDT , WP , WP$ , WRB , XX , _SP , dblSpecialApostrophe , normDollar , normDblApostrophe , normComma , norm-LRB- , norm-RRB- , normDot , normColon , normADD , normAFX , normCC , normCD , normDT , normEX , normFW , normHYPH , normIN , normJJ , normJJR , normJJS , normLS , normMD , normNFP , normNN , normNNP , normNNPS , normNNS , normPDT , normPOS , normPRP , normPRP$ , normRB , normRBR , normRBS , normRP , normSYM , normTO , normUH , normVB , normVBD , normVBG , normVBN , normVBP , normVBZ , normWDT , normWP , normWP$ , normWRB , normXX , norm_SP , normDblSpecialApostrophe , WC , Analytic , Clout , Authentic , Tone , WPS , Sixltr , Dic , function , pronoun , ppron , i , we , you , shehe , they , ipron , article , prep , auxverb , adverb , conj , negate , verb , adj , compare , interrog , number , quant , affect , posemo , negemo , anx , anger , sad , social , family , friend , female , male , cogproc , insight , cause , discrep , tentat , certain , differ , percept , see , hear , feel , bio , body , health , sexual , ingest , drives , affiliation , achieve , power , reward , risk , focuspast , focuspresent , focusfuture , relativ , motion , space , time , work , leisure , home , money , relig , death , informal , swear , netspeak , assent , nonflu , filler , AllPunc , Period , Comma , Colon , SemiC , QMark , Exclam , Dash , Quote , Apostro , Parenth , OtherP , norm_WC , norm_Analytic , norm_Clout , norm_Authentic , norm_Tone , norm_WPS , norm_Sixltr , norm_Dic , norm_function , norm_pronoun , norm_ppron , norm_i , norm_we , norm_you , norm_shehe , norm_they , norm_ipron , norm_article , norm_prep , norm_auxverb , norm_adverb , norm_conj , norm_negate , norm_verb , norm_adj , norm_compare , norm_interrog , norm_number , norm_quant , norm_affect , norm_posemo , norm_negemo , norm_anx , norm_anger , norm_sad , norm_social , norm_family , norm_friend , norm_female , norm_male , norm_cogproc , norm_insight , norm_cause , norm_discrep , norm_tentat , norm_certain , norm_differ , norm_percept , norm_see , norm_hear , norm_feel , norm_bio , norm_body , norm_health , norm_sexual , norm_ingest , norm_drives , norm_affiliation , norm_achieve , norm_power , norm_reward , norm_risk , norm_focuspast , norm_focuspresent , norm_focusfuture , norm_relativ , norm_motion , norm_space , norm_time , norm_work , norm_leisure , norm_home , norm_money , norm_relig , norm_death , norm_informal , norm_swear , norm_netspeak , norm_assent , norm_nonflu , norm_filler , norm_AllPunc , norm_Period , norm_Comma , norm_Colon , norm_SemiC , norm_QMark , norm_Exclam , norm_Dash , norm_Quote , norm_Apostro , norm_Parenth , norm_OtherP , totalTweets , totalRetweets , totalLikes , totalReplies , span , countWeekDay , countWeekEnd , countMonday , countTuesday , countWednesday , countThursday , countFriday , countSaturday , countSunday , avgTimeBetweenTweets , avgTimeOfNextTweet , timeAbsBin1count0to6 , timeAbsBin2count6to12 , timeAbsBin3count12to18 , timeAbsBin4count18to24 , timeAbsBin5count24plus , avgTweetsPerUniqUser , normTotalTweets , normTotalRetweets , normTotalLikes , normTotalReplies , normCountWeekDay , normCountWeekEnd , normCountMonday , normCountTuesday , normCountWednesday , normCountThursday , normCountFriday , normCountSaturday , normCountSunday , normAvgTweetsPerUniqUser , normTimeAbsBin1count0to6 , normTimeAbsBin2count6to12 , normTimeAbsBin3count12to18 , normTimeAbsBin4count18to24 , normTimeAbsBin5count24plus , normSpan , normAvgTimeBetweenTweets , normAvgTimeOfNextTweet , normPercentWeekDayByTotalTweets , normPercentWeekEndByTotalTweets , normPercentMondayByTotalTweets , normPercentTuesdayByTotalTweets , normPercentWednesdayByTotalTweets , normPercentThursdayByTotalTweets , normPercentFridayByTotalTweets , normPercentSaturdayByTotalTweets , normPercentSundayByTotalTweets , full_text_Flesch_reading_ease , full_text_Flesch_kincaid_grade , full_text_smog , full_text_gunning_fog , full_text_words_per_sentence , full_text_capitalized_words , full_text_lexicon , full_text_urls_counts , full_text_long_words , full_text_syllables , full_text_stop_words , full_text_sentences , full_text_linsear_write , full_text_automated_readability , full_text_characters_total , full_text_coleman_liax , full_text_difficult_words , full_text_words_total , norm_full_text_Flesch_reading_ease , norm_full_text_Flesch_kincaid_grade , norm_full_text_smog , norm_full_text_gunning_fog , norm_full_text_words_per_sentence , norm_full_text_capitalized_words , norm_full_text_lexicon , norm_full_text_urls_counts , norm_full_text_long_words , norm_full_text_syllables , norm_full_text_stop_words , norm_full_text_sentences , norm_full_text_linsear_write , norm_full_text_automated_readability , norm_full_text_characters_total , norm_full_text_coleman_liax , norm_full_text_difficult_words , norm_full_text_words_total , \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# At this stage domain and urlType exist in multiple files, but we want to retain only the M file ones.\n",
    "# delete the domain repetition and only retain it in the T file\n",
    "del dfTempL['domain']\n",
    "del dfTempR['domain']\n",
    "del dfTempR['urlType']\n",
    "# T file does not have domain.\n",
    "#\n",
    "dfTempOut = pd.merge(dfTempM, dfTempL, on=['url'])\n",
    "dfTempOut1 = pd.merge(dfTempOut, dfTempT, on=['url'])\n",
    "del dfTempOut\n",
    "df_ALL_Out_WITH_norm = pd.merge(dfTempOut1, dfTempR, on=['url'])\n",
    "del dfTempOut1\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#\n",
    "# see the columns for each of the files\n",
    "#\n",
    "outColStr = \"\"\n",
    "for col in list(df_ALL_Out_WITH_norm) :\n",
    "    outColStr += col + ' , '\n",
    "print(f\"df_ALL_Out_WITH_norm features = \\n{outColStr}\\n\")\n",
    "#\n",
    "######## temp code  ###   temp code   ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_ALL_Out_WITH_norm features after updation =\n",
      "domain , url , urlType , M_dollar , M_dblApostrophe , M_comma , M_-LRB- , M_-RRB- , M_dot , M_colon , M_ADD , M_AFX , M_CC , M_CD , M_DT , M_EX , M_FW , M_HYPH , M_IN , M_JJ , M_JJR , M_JJS , M_LS , M_MD , M_NFP , M_NN , M_NNP , M_NNPS , M_NNS , M_PDT , M_POS , M_PRP , M_PRP$ , M_RB , M_RBR , M_RBS , M_RP , M_SYM , M_TO , M_UH , M_VB , M_VBD , M_VBG , M_VBN , M_VBP , M_VBZ , M_WDT , M_WP , M_WP$ , M_WRB , M_XX , M__SP , M_dblSpecialApostrophe , M_normDollar , M_normDblApostrophe , M_normComma , M_norm-LRB- , M_norm-RRB- , M_normDot , M_normColon , M_normADD , M_normAFX , M_normCC , M_normCD , M_normDT , M_normEX , M_normFW , M_normHYPH , M_normIN , M_normJJ , M_normJJR , M_normJJS , M_normLS , M_normMD , M_normNFP , M_normNN , M_normNNP , M_normNNPS , M_normNNS , M_normPDT , M_normPOS , M_normPRP , M_normPRP$ , M_normRB , M_normRBR , M_normRBS , M_normRP , M_normSYM , M_normTO , M_normUH , M_normVB , M_normVBD , M_normVBG , M_normVBN , M_normVBP , M_normVBZ , M_normWDT , M_normWP , M_normWP$ , M_normWRB , M_normXX , M_norm_SP , M_normDblSpecialApostrophe , L_WC , L_Analytic , L_Clout , L_Authentic , L_Tone , L_WPS , L_Sixltr , L_Dic , L_function , L_pronoun , L_ppron , L_i , L_we , L_you , L_shehe , L_they , L_ipron , L_article , L_prep , L_auxverb , L_adverb , L_conj , L_negate , L_verb , L_adj , L_compare , L_interrog , L_number , L_quant , L_affect , L_posemo , L_negemo , L_anx , L_anger , L_sad , L_social , L_family , L_friend , L_female , L_male , L_cogproc , L_insight , L_cause , L_discrep , L_tentat , L_certain , L_differ , L_percept , L_see , L_hear , L_feel , L_bio , L_body , L_health , L_sexual , L_ingest , L_drives , L_affiliation , L_achieve , L_power , L_reward , L_risk , L_focuspast , L_focuspresent , L_focusfuture , L_relativ , L_motion , L_space , L_time , L_work , L_leisure , L_home , L_money , L_relig , L_death , L_informal , L_swear , L_netspeak , L_assent , L_nonflu , L_filler , L_AllPunc , L_Period , L_Comma , L_Colon , L_SemiC , L_QMark , L_Exclam , L_Dash , L_Quote , L_Apostro , L_Parenth , L_OtherP , L_norm_WC , L_norm_Analytic , L_norm_Clout , L_norm_Authentic , L_norm_Tone , L_norm_WPS , L_norm_Sixltr , L_norm_Dic , L_norm_function , L_norm_pronoun , L_norm_ppron , L_norm_i , L_norm_we , L_norm_you , L_norm_shehe , L_norm_they , L_norm_ipron , L_norm_article , L_norm_prep , L_norm_auxverb , L_norm_adverb , L_norm_conj , L_norm_negate , L_norm_verb , L_norm_adj , L_norm_compare , L_norm_interrog , L_norm_number , L_norm_quant , L_norm_affect , L_norm_posemo , L_norm_negemo , L_norm_anx , L_norm_anger , L_norm_sad , L_norm_social , L_norm_family , L_norm_friend , L_norm_female , L_norm_male , L_norm_cogproc , L_norm_insight , L_norm_cause , L_norm_discrep , L_norm_tentat , L_norm_certain , L_norm_differ , L_norm_percept , L_norm_see , L_norm_hear , L_norm_feel , L_norm_bio , L_norm_body , L_norm_health , L_norm_sexual , L_norm_ingest , L_norm_drives , L_norm_affiliation , L_norm_achieve , L_norm_power , L_norm_reward , L_norm_risk , L_norm_focuspast , L_norm_focuspresent , L_norm_focusfuture , L_norm_relativ , L_norm_motion , L_norm_space , L_norm_time , L_norm_work , L_norm_leisure , L_norm_home , L_norm_money , L_norm_relig , L_norm_death , L_norm_informal , L_norm_swear , L_norm_netspeak , L_norm_assent , L_norm_nonflu , L_norm_filler , L_norm_AllPunc , L_norm_Period , L_norm_Comma , L_norm_Colon , L_norm_SemiC , L_norm_QMark , L_norm_Exclam , L_norm_Dash , L_norm_Quote , L_norm_Apostro , L_norm_Parenth , L_norm_OtherP , T_totalTweets , T_totalRetweets , T_totalLikes , T_totalReplies , T_span , T_countWeekDay , T_countWeekEnd , T_countMonday , T_countTuesday , T_countWednesday , T_countThursday , T_countFriday , T_countSaturday , T_countSunday , T_avgTimeBetweenTweets , T_avgTimeOfNextTweet , T_timeAbsBin1count0to6 , T_timeAbsBin2count6to12 , T_timeAbsBin3count12to18 , T_timeAbsBin4count18to24 , T_timeAbsBin5count24plus , T_avgTweetsPerUniqUser , T_normTotalTweets , T_normTotalRetweets , T_normTotalLikes , T_normTotalReplies , T_normCountWeekDay , T_normCountWeekEnd , T_normCountMonday , T_normCountTuesday , T_normCountWednesday , T_normCountThursday , T_normCountFriday , T_normCountSaturday , T_normCountSunday , T_normAvgTweetsPerUniqUser , T_normTimeAbsBin1count0to6 , T_normTimeAbsBin2count6to12 , T_normTimeAbsBin3count12to18 , T_normTimeAbsBin4count18to24 , T_normTimeAbsBin5count24plus , T_normSpan , T_normAvgTimeBetweenTweets , T_normAvgTimeOfNextTweet , T_normPercentWeekDayByTotalTweets , T_normPercentWeekEndByTotalTweets , T_normPercentMondayByTotalTweets , T_normPercentTuesdayByTotalTweets , T_normPercentWednesdayByTotalTweets , T_normPercentThursdayByTotalTweets , T_normPercentFridayByTotalTweets , T_normPercentSaturdayByTotalTweets , T_normPercentSundayByTotalTweets , R_full_text_Flesch_reading_ease , R_full_text_Flesch_kincaid_grade , R_full_text_smog , R_full_text_gunning_fog , R_full_text_words_per_sentence , R_full_text_capitalized_words , R_full_text_lexicon , R_full_text_urls_counts , R_full_text_long_words , R_full_text_syllables , R_full_text_stop_words , R_full_text_sentences , R_full_text_linsear_write , R_full_text_automated_readability , R_full_text_characters_total , R_full_text_coleman_liax , R_full_text_difficult_words , R_full_text_words_total , R_norm_full_text_Flesch_reading_ease , R_norm_full_text_Flesch_kincaid_grade , R_norm_full_text_smog , R_norm_full_text_gunning_fog , R_norm_full_text_words_per_sentence , R_norm_full_text_capitalized_words , R_norm_full_text_lexicon , R_norm_full_text_urls_counts , R_norm_full_text_long_words , R_norm_full_text_syllables , R_norm_full_text_stop_words , R_norm_full_text_sentences , R_norm_full_text_linsear_write , R_norm_full_text_automated_readability , R_norm_full_text_characters_total , R_norm_full_text_coleman_liax , R_norm_full_text_difficult_words , R_norm_full_text_words_total , \n",
      "\n",
      "df_ALL_Out_NO_norm features =\n",
      "domain , url , urlType , M_dollar , M_dblApostrophe , M_comma , M_-LRB- , M_-RRB- , M_dot , M_colon , M_ADD , M_AFX , M_CC , M_CD , M_DT , M_EX , M_FW , M_HYPH , M_IN , M_JJ , M_JJR , M_JJS , M_LS , M_MD , M_NFP , M_NN , M_NNP , M_NNPS , M_NNS , M_PDT , M_POS , M_PRP , M_PRP$ , M_RB , M_RBR , M_RBS , M_RP , M_SYM , M_TO , M_UH , M_VB , M_VBD , M_VBG , M_VBN , M_VBP , M_VBZ , M_WDT , M_WP , M_WP$ , M_WRB , M_XX , M__SP , M_dblSpecialApostrophe , L_WC , L_Analytic , L_Clout , L_Authentic , L_Tone , L_WPS , L_Sixltr , L_Dic , L_function , L_pronoun , L_ppron , L_i , L_we , L_you , L_shehe , L_they , L_ipron , L_article , L_prep , L_auxverb , L_adverb , L_conj , L_negate , L_verb , L_adj , L_compare , L_interrog , L_number , L_quant , L_affect , L_posemo , L_negemo , L_anx , L_anger , L_sad , L_social , L_family , L_friend , L_female , L_male , L_cogproc , L_insight , L_cause , L_discrep , L_tentat , L_certain , L_differ , L_percept , L_see , L_hear , L_feel , L_bio , L_body , L_health , L_sexual , L_ingest , L_drives , L_affiliation , L_achieve , L_power , L_reward , L_risk , L_focuspast , L_focuspresent , L_focusfuture , L_relativ , L_motion , L_space , L_time , L_work , L_leisure , L_home , L_money , L_relig , L_death , L_informal , L_swear , L_netspeak , L_assent , L_nonflu , L_filler , L_AllPunc , L_Period , L_Comma , L_Colon , L_SemiC , L_QMark , L_Exclam , L_Dash , L_Quote , L_Apostro , L_Parenth , L_OtherP , T_totalTweets , T_totalRetweets , T_totalLikes , T_totalReplies , T_span , T_countWeekDay , T_countWeekEnd , T_countMonday , T_countTuesday , T_countWednesday , T_countThursday , T_countFriday , T_countSaturday , T_countSunday , T_avgTimeBetweenTweets , T_avgTimeOfNextTweet , T_timeAbsBin1count0to6 , T_timeAbsBin2count6to12 , T_timeAbsBin3count12to18 , T_timeAbsBin4count18to24 , T_timeAbsBin5count24plus , T_avgTweetsPerUniqUser , R_full_text_Flesch_reading_ease , R_full_text_Flesch_kincaid_grade , R_full_text_smog , R_full_text_gunning_fog , R_full_text_words_per_sentence , R_full_text_capitalized_words , R_full_text_lexicon , R_full_text_urls_counts , R_full_text_long_words , R_full_text_syllables , R_full_text_stop_words , R_full_text_sentences , R_full_text_linsear_write , R_full_text_automated_readability , R_full_text_characters_total , R_full_text_coleman_liax , R_full_text_difficult_words , R_full_text_words_total , \n",
      "\n"
     ]
    }
   ],
   "source": [
    "colsList_ALL_WITH_norm = []\n",
    "# expecting the order of features as Morphological , LIWC (Psychological) , Twitter , Readability\n",
    "preTagList = ['' , 'M_' , 'L_' , 'T_' , 'R_']\n",
    "idxPreTagList = 0\n",
    "for col in list(df_ALL_Out_WITH_norm) :\n",
    "    if col == 'dollar':                            ## first column of M Morphological\n",
    "        idxPreTagList = 1\n",
    "    elif col == 'WC':                              ## first column of L LIWC\n",
    "        idxPreTagList = 2\n",
    "    elif col == 'totalTweets':                     ## first column of T Twitter\n",
    "        idxPreTagList = 3\n",
    "    elif col == 'full_text_Flesch_reading_ease':   ## first column of R Readability\n",
    "        idxPreTagList = 4\n",
    "    newCol = preTagList[idxPreTagList] + col\n",
    "    colsList_ALL_WITH_norm.append(newCol)\n",
    "#\n",
    "# Update the df_All_Out columns\n",
    "# Thus the All features WITH norm dataframe is ready\n",
    "#\n",
    "df_ALL_Out_WITH_norm.columns = colsList_ALL_WITH_norm\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#\n",
    "# see the updated column names for all the features\n",
    "#\n",
    "outColStr = \"\"\n",
    "for col in list(df_ALL_Out_WITH_norm) :\n",
    "    outColStr += col + ' , '\n",
    "print(f\"df_ALL_Out_WITH_norm features after updation =\\n{outColStr}\\n\")\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#\n",
    "#\n",
    "########\n",
    "#\n",
    "# identify ALL features which are regular by dropping normalized features and write to CSV file\n",
    "colsList_ALL_NO_norm = []\n",
    "for col in list(df_ALL_Out_WITH_norm) :\n",
    "    if col in ['url' , 'domain' , 'urlType']: # these three must be retained\n",
    "        colsList_ALL_NO_norm.append(col)\n",
    "    elif \"norm\" not in col :\n",
    "        colsList_ALL_NO_norm.append(col)\n",
    "#\n",
    "df_ALL_Out_NO_norm = df_ALL_Out_WITH_norm[colsList_ALL_NO_norm]\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#\n",
    "# check the columns\n",
    "#\n",
    "outColStr = \"\"\n",
    "for col in list(colsList_ALL_NO_norm) :\n",
    "    outColStr += col + ' , '\n",
    "print(f\"df_ALL_Out_NO_norm features =\\n{outColStr}\\n\")\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create all the data files to be used for training or testing new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "folderPath = '/home/rohit/SRH/CaseStudy1/dataDemo/dataFiles/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write to CSV: All features WITH norm    and   other file for All features WITHOUT norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ALL_Out_WITH_norm.to_csv(folderPath + 'All_Features_WITH_norm.csv' , index = True)\n",
    "df_ALL_Out_NO_norm.to_csv(folderPath + 'All_Features_NO_norm.csv' , index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_ALL_Out_WITH_norm , df_ALL_Out_NO_norm, dfTempM , dfTempL , dfTempT , dfTempR\n",
    "del colsList_ALL_WITH_norm, colsList_ALL_NO_norm, outColStr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in the All_Features file into a dataframe and process from there to create\n",
    "#      the individual group feature files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ALL_Out_WITH_norm = pd.read_csv(folderPath + 'All_Features_WITH_norm.csv' , index_col = 0, sep = ',', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 378)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ALL_Out_WITH_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_ALL_Out_WITH_norm features after updation =\n",
      "domain , url , urlType , M_dollar , M_dblApostrophe , M_comma , M_-LRB- , M_-RRB- , M_dot , M_colon , M_ADD , M_AFX , M_CC , M_CD , M_DT , M_EX , M_FW , M_HYPH , M_IN , M_JJ , M_JJR , M_JJS , M_LS , M_MD , M_NFP , M_NN , M_NNP , M_NNPS , M_NNS , M_PDT , M_POS , M_PRP , M_PRP$ , M_RB , M_RBR , M_RBS , M_RP , M_SYM , M_TO , M_UH , M_VB , M_VBD , M_VBG , M_VBN , M_VBP , M_VBZ , M_WDT , M_WP , M_WP$ , M_WRB , M_XX , M__SP , M_dblSpecialApostrophe , M_normDollar , M_normDblApostrophe , M_normComma , M_norm-LRB- , M_norm-RRB- , M_normDot , M_normColon , M_normADD , M_normAFX , M_normCC , M_normCD , M_normDT , M_normEX , M_normFW , M_normHYPH , M_normIN , M_normJJ , M_normJJR , M_normJJS , M_normLS , M_normMD , M_normNFP , M_normNN , M_normNNP , M_normNNPS , M_normNNS , M_normPDT , M_normPOS , M_normPRP , M_normPRP$ , M_normRB , M_normRBR , M_normRBS , M_normRP , M_normSYM , M_normTO , M_normUH , M_normVB , M_normVBD , M_normVBG , M_normVBN , M_normVBP , M_normVBZ , M_normWDT , M_normWP , M_normWP$ , M_normWRB , M_normXX , M_norm_SP , M_normDblSpecialApostrophe , L_WC , L_Analytic , L_Clout , L_Authentic , L_Tone , L_WPS , L_Sixltr , L_Dic , L_function , L_pronoun , L_ppron , L_i , L_we , L_you , L_shehe , L_they , L_ipron , L_article , L_prep , L_auxverb , L_adverb , L_conj , L_negate , L_verb , L_adj , L_compare , L_interrog , L_number , L_quant , L_affect , L_posemo , L_negemo , L_anx , L_anger , L_sad , L_social , L_family , L_friend , L_female , L_male , L_cogproc , L_insight , L_cause , L_discrep , L_tentat , L_certain , L_differ , L_percept , L_see , L_hear , L_feel , L_bio , L_body , L_health , L_sexual , L_ingest , L_drives , L_affiliation , L_achieve , L_power , L_reward , L_risk , L_focuspast , L_focuspresent , L_focusfuture , L_relativ , L_motion , L_space , L_time , L_work , L_leisure , L_home , L_money , L_relig , L_death , L_informal , L_swear , L_netspeak , L_assent , L_nonflu , L_filler , L_AllPunc , L_Period , L_Comma , L_Colon , L_SemiC , L_QMark , L_Exclam , L_Dash , L_Quote , L_Apostro , L_Parenth , L_OtherP , L_norm_WC , L_norm_Analytic , L_norm_Clout , L_norm_Authentic , L_norm_Tone , L_norm_WPS , L_norm_Sixltr , L_norm_Dic , L_norm_function , L_norm_pronoun , L_norm_ppron , L_norm_i , L_norm_we , L_norm_you , L_norm_shehe , L_norm_they , L_norm_ipron , L_norm_article , L_norm_prep , L_norm_auxverb , L_norm_adverb , L_norm_conj , L_norm_negate , L_norm_verb , L_norm_adj , L_norm_compare , L_norm_interrog , L_norm_number , L_norm_quant , L_norm_affect , L_norm_posemo , L_norm_negemo , L_norm_anx , L_norm_anger , L_norm_sad , L_norm_social , L_norm_family , L_norm_friend , L_norm_female , L_norm_male , L_norm_cogproc , L_norm_insight , L_norm_cause , L_norm_discrep , L_norm_tentat , L_norm_certain , L_norm_differ , L_norm_percept , L_norm_see , L_norm_hear , L_norm_feel , L_norm_bio , L_norm_body , L_norm_health , L_norm_sexual , L_norm_ingest , L_norm_drives , L_norm_affiliation , L_norm_achieve , L_norm_power , L_norm_reward , L_norm_risk , L_norm_focuspast , L_norm_focuspresent , L_norm_focusfuture , L_norm_relativ , L_norm_motion , L_norm_space , L_norm_time , L_norm_work , L_norm_leisure , L_norm_home , L_norm_money , L_norm_relig , L_norm_death , L_norm_informal , L_norm_swear , L_norm_netspeak , L_norm_assent , L_norm_nonflu , L_norm_filler , L_norm_AllPunc , L_norm_Period , L_norm_Comma , L_norm_Colon , L_norm_SemiC , L_norm_QMark , L_norm_Exclam , L_norm_Dash , L_norm_Quote , L_norm_Apostro , L_norm_Parenth , L_norm_OtherP , T_totalTweets , T_totalRetweets , T_totalLikes , T_totalReplies , T_span , T_countWeekDay , T_countWeekEnd , T_countMonday , T_countTuesday , T_countWednesday , T_countThursday , T_countFriday , T_countSaturday , T_countSunday , T_avgTimeBetweenTweets , T_avgTimeOfNextTweet , T_timeAbsBin1count0to6 , T_timeAbsBin2count6to12 , T_timeAbsBin3count12to18 , T_timeAbsBin4count18to24 , T_timeAbsBin5count24plus , T_avgTweetsPerUniqUser , T_normTotalTweets , T_normTotalRetweets , T_normTotalLikes , T_normTotalReplies , T_normCountWeekDay , T_normCountWeekEnd , T_normCountMonday , T_normCountTuesday , T_normCountWednesday , T_normCountThursday , T_normCountFriday , T_normCountSaturday , T_normCountSunday , T_normAvgTweetsPerUniqUser , T_normTimeAbsBin1count0to6 , T_normTimeAbsBin2count6to12 , T_normTimeAbsBin3count12to18 , T_normTimeAbsBin4count18to24 , T_normTimeAbsBin5count24plus , T_normSpan , T_normAvgTimeBetweenTweets , T_normAvgTimeOfNextTweet , T_normPercentWeekDayByTotalTweets , T_normPercentWeekEndByTotalTweets , T_normPercentMondayByTotalTweets , T_normPercentTuesdayByTotalTweets , T_normPercentWednesdayByTotalTweets , T_normPercentThursdayByTotalTweets , T_normPercentFridayByTotalTweets , T_normPercentSaturdayByTotalTweets , T_normPercentSundayByTotalTweets , R_full_text_Flesch_reading_ease , R_full_text_Flesch_kincaid_grade , R_full_text_smog , R_full_text_gunning_fog , R_full_text_words_per_sentence , R_full_text_capitalized_words , R_full_text_lexicon , R_full_text_urls_counts , R_full_text_long_words , R_full_text_syllables , R_full_text_stop_words , R_full_text_sentences , R_full_text_linsear_write , R_full_text_automated_readability , R_full_text_characters_total , R_full_text_coleman_liax , R_full_text_difficult_words , R_full_text_words_total , R_norm_full_text_Flesch_reading_ease , R_norm_full_text_Flesch_kincaid_grade , R_norm_full_text_smog , R_norm_full_text_gunning_fog , R_norm_full_text_words_per_sentence , R_norm_full_text_capitalized_words , R_norm_full_text_lexicon , R_norm_full_text_urls_counts , R_norm_full_text_long_words , R_norm_full_text_syllables , R_norm_full_text_stop_words , R_norm_full_text_sentences , R_norm_full_text_linsear_write , R_norm_full_text_automated_readability , R_norm_full_text_characters_total , R_norm_full_text_coleman_liax , R_norm_full_text_difficult_words , R_norm_full_text_words_total , \n",
      "\n"
     ]
    }
   ],
   "source": [
    "outColStr = \"\"\n",
    "for col in list(df_ALL_Out_WITH_norm) :\n",
    "    outColStr += col + ' , '\n",
    "print(f\"df_ALL_Out_WITH_norm features after updation =\\n{outColStr}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# identify L features and write to CSV file\n",
    "### this is done for two files:\n",
    "###        one WITH normalized values included\n",
    "###        other is for NO normalized values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_L_Out_WITH_norm features =\n",
      "domain , url , urlType , L_WC , L_Analytic , L_Clout , L_Authentic , L_Tone , L_WPS , L_Sixltr , L_Dic , L_function , L_pronoun , L_ppron , L_i , L_we , L_you , L_shehe , L_they , L_ipron , L_article , L_prep , L_auxverb , L_adverb , L_conj , L_negate , L_verb , L_adj , L_compare , L_interrog , L_number , L_quant , L_affect , L_posemo , L_negemo , L_anx , L_anger , L_sad , L_social , L_family , L_friend , L_female , L_male , L_cogproc , L_insight , L_cause , L_discrep , L_tentat , L_certain , L_differ , L_percept , L_see , L_hear , L_feel , L_bio , L_body , L_health , L_sexual , L_ingest , L_drives , L_affiliation , L_achieve , L_power , L_reward , L_risk , L_focuspast , L_focuspresent , L_focusfuture , L_relativ , L_motion , L_space , L_time , L_work , L_leisure , L_home , L_money , L_relig , L_death , L_informal , L_swear , L_netspeak , L_assent , L_nonflu , L_filler , L_AllPunc , L_Period , L_Comma , L_Colon , L_SemiC , L_QMark , L_Exclam , L_Dash , L_Quote , L_Apostro , L_Parenth , L_OtherP , L_norm_WC , L_norm_Analytic , L_norm_Clout , L_norm_Authentic , L_norm_Tone , L_norm_WPS , L_norm_Sixltr , L_norm_Dic , L_norm_function , L_norm_pronoun , L_norm_ppron , L_norm_i , L_norm_we , L_norm_you , L_norm_shehe , L_norm_they , L_norm_ipron , L_norm_article , L_norm_prep , L_norm_auxverb , L_norm_adverb , L_norm_conj , L_norm_negate , L_norm_verb , L_norm_adj , L_norm_compare , L_norm_interrog , L_norm_number , L_norm_quant , L_norm_affect , L_norm_posemo , L_norm_negemo , L_norm_anx , L_norm_anger , L_norm_sad , L_norm_social , L_norm_family , L_norm_friend , L_norm_female , L_norm_male , L_norm_cogproc , L_norm_insight , L_norm_cause , L_norm_discrep , L_norm_tentat , L_norm_certain , L_norm_differ , L_norm_percept , L_norm_see , L_norm_hear , L_norm_feel , L_norm_bio , L_norm_body , L_norm_health , L_norm_sexual , L_norm_ingest , L_norm_drives , L_norm_affiliation , L_norm_achieve , L_norm_power , L_norm_reward , L_norm_risk , L_norm_focuspast , L_norm_focuspresent , L_norm_focusfuture , L_norm_relativ , L_norm_motion , L_norm_space , L_norm_time , L_norm_work , L_norm_leisure , L_norm_home , L_norm_money , L_norm_relig , L_norm_death , L_norm_informal , L_norm_swear , L_norm_netspeak , L_norm_assent , L_norm_nonflu , L_norm_filler , L_norm_AllPunc , L_norm_Period , L_norm_Comma , L_norm_Colon , L_norm_SemiC , L_norm_QMark , L_norm_Exclam , L_norm_Dash , L_norm_Quote , L_norm_Apostro , L_norm_Parenth , L_norm_OtherP , \n",
      "\n",
      "df_L_Out_NO_norm features =\n",
      "domain , url , urlType , L_WC , L_Analytic , L_Clout , L_Authentic , L_Tone , L_WPS , L_Sixltr , L_Dic , L_function , L_pronoun , L_ppron , L_i , L_we , L_you , L_shehe , L_they , L_ipron , L_article , L_prep , L_auxverb , L_adverb , L_conj , L_negate , L_verb , L_adj , L_compare , L_interrog , L_number , L_quant , L_affect , L_posemo , L_negemo , L_anx , L_anger , L_sad , L_social , L_family , L_friend , L_female , L_male , L_cogproc , L_insight , L_cause , L_discrep , L_tentat , L_certain , L_differ , L_percept , L_see , L_hear , L_feel , L_bio , L_body , L_health , L_sexual , L_ingest , L_drives , L_affiliation , L_achieve , L_power , L_reward , L_risk , L_focuspast , L_focuspresent , L_focusfuture , L_relativ , L_motion , L_space , L_time , L_work , L_leisure , L_home , L_money , L_relig , L_death , L_informal , L_swear , L_netspeak , L_assent , L_nonflu , L_filler , L_AllPunc , L_Period , L_Comma , L_Colon , L_SemiC , L_QMark , L_Exclam , L_Dash , L_Quote , L_Apostro , L_Parenth , L_OtherP , \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# identify all L features and write to CSV file\n",
    "colsList_L_WITH_norm = []\n",
    "for col in list(df_ALL_Out_WITH_norm) :\n",
    "    if col in ['url' , 'domain' , 'urlType']: # these three must be retained\n",
    "        colsList_L_WITH_norm.append(col)\n",
    "    elif col.startswith('L_'):\n",
    "        colsList_L_WITH_norm.append(col)\n",
    "#\n",
    "df_L_Out_WITH_norm = df_ALL_Out_WITH_norm[colsList_L_WITH_norm]\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#\n",
    "# check the columns\n",
    "#\n",
    "outColStr = \"\"\n",
    "for col in list(df_L_Out_WITH_norm) :\n",
    "    outColStr += col + ' , '\n",
    "print(f\"df_L_Out_WITH_norm features =\\n{outColStr}\\n\")\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#\n",
    "#\n",
    "########\n",
    "#\n",
    "# identify L features which are regular by dropping normalized features and write to CSV file\n",
    "colsList_L_NO_norm = []\n",
    "for col in list(df_ALL_Out_WITH_norm) :\n",
    "    if col in ['url' , 'domain' , 'urlType']: # these three must be retained\n",
    "        colsList_L_NO_norm.append(col)\n",
    "    elif col.startswith('L_'):\n",
    "        if \"norm\" not in col :\n",
    "            colsList_L_NO_norm.append(col)\n",
    "#\n",
    "df_L_Out_NO_norm = df_ALL_Out_WITH_norm[colsList_L_NO_norm]\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#\n",
    "# check the columns\n",
    "#\n",
    "outColStr = \"\"\n",
    "for col in list(df_L_Out_NO_norm) :\n",
    "    outColStr += col + ' , '\n",
    "print(f\"df_L_Out_NO_norm features =\\n{outColStr}\\n\")\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write to CSV: L features WITH norm    and   other file for L features WITHOUT norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_L_Out_WITH_norm.to_csv(folderPath + 'L_Features_WITH_norm.csv' , index = True)\n",
    "del df_L_Out_WITH_norm\n",
    "df_L_Out_NO_norm.to_csv(folderPath + 'L_Features_NO_norm.csv' , index = True)\n",
    "del df_L_Out_NO_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# identify M features and write to CSV file\n",
    "### this is done for two files:\n",
    "###        one WITH normalized values included\n",
    "###        other is for NO normalized values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_M_Out_WITH_norm features =\n",
      "domain , url , urlType , M_dollar , M_dblApostrophe , M_comma , M_-LRB- , M_-RRB- , M_dot , M_colon , M_ADD , M_AFX , M_CC , M_CD , M_DT , M_EX , M_FW , M_HYPH , M_IN , M_JJ , M_JJR , M_JJS , M_LS , M_MD , M_NFP , M_NN , M_NNP , M_NNPS , M_NNS , M_PDT , M_POS , M_PRP , M_PRP$ , M_RB , M_RBR , M_RBS , M_RP , M_SYM , M_TO , M_UH , M_VB , M_VBD , M_VBG , M_VBN , M_VBP , M_VBZ , M_WDT , M_WP , M_WP$ , M_WRB , M_XX , M__SP , M_dblSpecialApostrophe , M_normDollar , M_normDblApostrophe , M_normComma , M_norm-LRB- , M_norm-RRB- , M_normDot , M_normColon , M_normADD , M_normAFX , M_normCC , M_normCD , M_normDT , M_normEX , M_normFW , M_normHYPH , M_normIN , M_normJJ , M_normJJR , M_normJJS , M_normLS , M_normMD , M_normNFP , M_normNN , M_normNNP , M_normNNPS , M_normNNS , M_normPDT , M_normPOS , M_normPRP , M_normPRP$ , M_normRB , M_normRBR , M_normRBS , M_normRP , M_normSYM , M_normTO , M_normUH , M_normVB , M_normVBD , M_normVBG , M_normVBN , M_normVBP , M_normVBZ , M_normWDT , M_normWP , M_normWP$ , M_normWRB , M_normXX , M_norm_SP , M_normDblSpecialApostrophe , \n",
      "\n",
      "df_M_Out_NO_norm features =\n",
      "domain , url , urlType , M_dollar , M_dblApostrophe , M_comma , M_-LRB- , M_-RRB- , M_dot , M_colon , M_ADD , M_AFX , M_CC , M_CD , M_DT , M_EX , M_FW , M_HYPH , M_IN , M_JJ , M_JJR , M_JJS , M_LS , M_MD , M_NFP , M_NN , M_NNP , M_NNPS , M_NNS , M_PDT , M_POS , M_PRP , M_PRP$ , M_RB , M_RBR , M_RBS , M_RP , M_SYM , M_TO , M_UH , M_VB , M_VBD , M_VBG , M_VBN , M_VBP , M_VBZ , M_WDT , M_WP , M_WP$ , M_WRB , M_XX , M__SP , M_dblSpecialApostrophe , \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# identify all M features and write to CSV file\n",
    "colsList_M_WITH_norm = []\n",
    "for col in list(df_ALL_Out_WITH_norm) :\n",
    "    if col in ['url' , 'domain' , 'urlType']: # these three must be retained\n",
    "        colsList_M_WITH_norm.append(col)\n",
    "    elif col.startswith('M_'):\n",
    "        colsList_M_WITH_norm.append(col)\n",
    "#\n",
    "df_M_Out_WITH_norm = df_ALL_Out_WITH_norm[colsList_M_WITH_norm]\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#\n",
    "# check the columns\n",
    "#\n",
    "outColStr = \"\"\n",
    "for col in list(df_M_Out_WITH_norm) :\n",
    "    outColStr += col + ' , '\n",
    "print(f\"df_M_Out_WITH_norm features =\\n{outColStr}\\n\")\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#\n",
    "#\n",
    "########\n",
    "#\n",
    "# identify M features which are regular by dropping normalized features and write to CSV file\n",
    "colsList_M_NO_norm = []\n",
    "for col in list(df_ALL_Out_WITH_norm) :\n",
    "    if col in ['url' , 'domain' , 'urlType']: # these three must be retained\n",
    "        colsList_M_NO_norm.append(col)\n",
    "    elif col.startswith('M_'):\n",
    "        if \"norm\" not in col :\n",
    "            colsList_M_NO_norm.append(col)\n",
    "#\n",
    "df_M_Out_NO_norm = df_ALL_Out_WITH_norm[colsList_M_NO_norm]\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#\n",
    "# check the columns\n",
    "#\n",
    "outColStr = \"\"\n",
    "for col in list(df_M_Out_NO_norm) :\n",
    "    outColStr += col + ' , '\n",
    "print(f\"df_M_Out_NO_norm features =\\n{outColStr}\\n\")\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write to CSV: M features WITH norm    and   other file for M features WITHOUT norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_M_Out_WITH_norm.to_csv(folderPath + 'M_Features_WITH_norm.csv' , index = True)\n",
    "del df_M_Out_WITH_norm\n",
    "df_M_Out_NO_norm.to_csv(folderPath + 'M_Features_NO_norm.csv' , index = True)\n",
    "del df_M_Out_NO_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# identify R features and write to CSV file\n",
    "### this is done for two files:\n",
    "###        one WITH normalized values included\n",
    "###        other is for NO normalized values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_R_Out_WITH_norm features =\n",
      "domain , url , urlType , R_full_text_Flesch_reading_ease , R_full_text_Flesch_kincaid_grade , R_full_text_smog , R_full_text_gunning_fog , R_full_text_words_per_sentence , R_full_text_capitalized_words , R_full_text_lexicon , R_full_text_urls_counts , R_full_text_long_words , R_full_text_syllables , R_full_text_stop_words , R_full_text_sentences , R_full_text_linsear_write , R_full_text_automated_readability , R_full_text_characters_total , R_full_text_coleman_liax , R_full_text_difficult_words , R_full_text_words_total , R_norm_full_text_Flesch_reading_ease , R_norm_full_text_Flesch_kincaid_grade , R_norm_full_text_smog , R_norm_full_text_gunning_fog , R_norm_full_text_words_per_sentence , R_norm_full_text_capitalized_words , R_norm_full_text_lexicon , R_norm_full_text_urls_counts , R_norm_full_text_long_words , R_norm_full_text_syllables , R_norm_full_text_stop_words , R_norm_full_text_sentences , R_norm_full_text_linsear_write , R_norm_full_text_automated_readability , R_norm_full_text_characters_total , R_norm_full_text_coleman_liax , R_norm_full_text_difficult_words , R_norm_full_text_words_total , \n",
      "\n",
      "df_R_Out_NO_norm features =\n",
      "domain , url , urlType , R_full_text_Flesch_reading_ease , R_full_text_Flesch_kincaid_grade , R_full_text_smog , R_full_text_gunning_fog , R_full_text_words_per_sentence , R_full_text_capitalized_words , R_full_text_lexicon , R_full_text_urls_counts , R_full_text_long_words , R_full_text_syllables , R_full_text_stop_words , R_full_text_sentences , R_full_text_linsear_write , R_full_text_automated_readability , R_full_text_characters_total , R_full_text_coleman_liax , R_full_text_difficult_words , R_full_text_words_total , \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# identify all R features and write to CSV file\n",
    "colsList_R_WITH_norm = []\n",
    "for col in list(df_ALL_Out_WITH_norm) :\n",
    "    if col in ['url' , 'domain' , 'urlType']: # these three must be retained\n",
    "        colsList_R_WITH_norm.append(col)\n",
    "    elif col.startswith('R_'):\n",
    "        colsList_R_WITH_norm.append(col)\n",
    "#\n",
    "df_R_Out_WITH_norm = df_ALL_Out_WITH_norm[colsList_R_WITH_norm]\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#\n",
    "# check the columns\n",
    "#\n",
    "outColStr = \"\"\n",
    "for col in list(df_R_Out_WITH_norm) :\n",
    "    outColStr += col + ' , '\n",
    "print(f\"df_R_Out_WITH_norm features =\\n{outColStr}\\n\")\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#\n",
    "#\n",
    "########\n",
    "#\n",
    "# identify R features which are regular by dropping normalized features and write to CSV file\n",
    "colsList_R_NO_norm = []\n",
    "for col in list(df_ALL_Out_WITH_norm) :\n",
    "    if col in ['url' , 'domain' , 'urlType']: # these three must be retained\n",
    "        colsList_R_NO_norm.append(col)\n",
    "    elif col.startswith('R_'):\n",
    "        if \"norm\" not in col :\n",
    "            colsList_R_NO_norm.append(col)\n",
    "#\n",
    "df_R_Out_NO_norm = df_ALL_Out_WITH_norm[colsList_R_NO_norm]\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#\n",
    "# check the columns\n",
    "#\n",
    "outColStr = \"\"\n",
    "for col in list(df_R_Out_NO_norm) :\n",
    "    outColStr += col + ' , '\n",
    "print(f\"df_R_Out_NO_norm features =\\n{outColStr}\\n\")\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_R_Out_WITH_norm.to_csv(folderPath + 'R_Features_WITH_norm.csv' , index = True)\n",
    "del df_R_Out_WITH_norm\n",
    "df_R_Out_NO_norm.to_csv(folderPath + 'R_Features_NO_norm.csv' , index = True)\n",
    "del df_R_Out_NO_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# identify T features and write to CSV file\n",
    "### this is done for two files:\n",
    "###        one WITH normalized values included\n",
    "###        other is for NO normalized values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_T_Out_WITH_norm features =\n",
      "domain , url , urlType , T_totalTweets , T_totalRetweets , T_totalLikes , T_totalReplies , T_span , T_countWeekDay , T_countWeekEnd , T_countMonday , T_countTuesday , T_countWednesday , T_countThursday , T_countFriday , T_countSaturday , T_countSunday , T_avgTimeBetweenTweets , T_avgTimeOfNextTweet , T_timeAbsBin1count0to6 , T_timeAbsBin2count6to12 , T_timeAbsBin3count12to18 , T_timeAbsBin4count18to24 , T_timeAbsBin5count24plus , T_avgTweetsPerUniqUser , T_normTotalTweets , T_normTotalRetweets , T_normTotalLikes , T_normTotalReplies , T_normCountWeekDay , T_normCountWeekEnd , T_normCountMonday , T_normCountTuesday , T_normCountWednesday , T_normCountThursday , T_normCountFriday , T_normCountSaturday , T_normCountSunday , T_normAvgTweetsPerUniqUser , T_normTimeAbsBin1count0to6 , T_normTimeAbsBin2count6to12 , T_normTimeAbsBin3count12to18 , T_normTimeAbsBin4count18to24 , T_normTimeAbsBin5count24plus , T_normSpan , T_normAvgTimeBetweenTweets , T_normAvgTimeOfNextTweet , T_normPercentWeekDayByTotalTweets , T_normPercentWeekEndByTotalTweets , T_normPercentMondayByTotalTweets , T_normPercentTuesdayByTotalTweets , T_normPercentWednesdayByTotalTweets , T_normPercentThursdayByTotalTweets , T_normPercentFridayByTotalTweets , T_normPercentSaturdayByTotalTweets , T_normPercentSundayByTotalTweets , \n",
      "\n",
      "df_T_Out_NO_norm features =\n",
      "domain , url , urlType , T_totalTweets , T_totalRetweets , T_totalLikes , T_totalReplies , T_span , T_countWeekDay , T_countWeekEnd , T_countMonday , T_countTuesday , T_countWednesday , T_countThursday , T_countFriday , T_countSaturday , T_countSunday , T_avgTimeBetweenTweets , T_avgTimeOfNextTweet , T_timeAbsBin1count0to6 , T_timeAbsBin2count6to12 , T_timeAbsBin3count12to18 , T_timeAbsBin4count18to24 , T_timeAbsBin5count24plus , T_avgTweetsPerUniqUser , \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# identify all T features and write to CSV file\n",
    "colsList_T_WITH_norm = []\n",
    "for col in list(df_ALL_Out_WITH_norm) :\n",
    "    if col in ['url' , 'domain' , 'urlType']: # these three must be retained\n",
    "        colsList_T_WITH_norm.append(col)\n",
    "    elif col.startswith('T_'):\n",
    "        colsList_T_WITH_norm.append(col)\n",
    "#\n",
    "df_T_Out_WITH_norm = df_ALL_Out_WITH_norm[colsList_T_WITH_norm]\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#\n",
    "# check the columns\n",
    "#\n",
    "outColStr = \"\"\n",
    "for col in list(df_T_Out_WITH_norm) :\n",
    "    outColStr += col + ' , '\n",
    "print(f\"df_T_Out_WITH_norm features =\\n{outColStr}\\n\")\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#\n",
    "#\n",
    "########\n",
    "#\n",
    "# identify T features which are regular by dropping normalized features and write to CSV file\n",
    "colsList_T_NO_norm = []\n",
    "for col in list(df_ALL_Out_WITH_norm) :\n",
    "    if col in ['url' , 'domain' , 'urlType']: # these three must be retained\n",
    "        colsList_T_NO_norm.append(col)\n",
    "    elif col.startswith('T_'):\n",
    "        if \"norm\" not in col :\n",
    "            colsList_T_NO_norm.append(col)\n",
    "#\n",
    "df_T_Out_NO_norm = df_ALL_Out_WITH_norm[colsList_T_NO_norm]\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#\n",
    "# check the columns\n",
    "#\n",
    "outColStr = \"\"\n",
    "for col in list(df_T_Out_NO_norm) :\n",
    "    outColStr += col + ' , '\n",
    "print(f\"df_T_Out_NO_norm features =\\n{outColStr}\\n\")\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_T_Out_WITH_norm.to_csv(folderPath + 'T_Features_WITH_norm.csv' , index = True)\n",
    "del df_T_Out_WITH_norm\n",
    "df_T_Out_NO_norm.to_csv(folderPath + 'T_Features_NO_norm.csv' , index = True)\n",
    "del df_T_Out_NO_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# identify MLR features and write to CSV file\n",
    "### this is done for two files:\n",
    "###        one WITH normalized values included\n",
    "###        other is for NO normalized values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_MLR_Out_WITH_norm features =\n",
      "domain , url , urlType , M_dollar , M_dblApostrophe , M_comma , M_-LRB- , M_-RRB- , M_dot , M_colon , M_ADD , M_AFX , M_CC , M_CD , M_DT , M_EX , M_FW , M_HYPH , M_IN , M_JJ , M_JJR , M_JJS , M_LS , M_MD , M_NFP , M_NN , M_NNP , M_NNPS , M_NNS , M_PDT , M_POS , M_PRP , M_PRP$ , M_RB , M_RBR , M_RBS , M_RP , M_SYM , M_TO , M_UH , M_VB , M_VBD , M_VBG , M_VBN , M_VBP , M_VBZ , M_WDT , M_WP , M_WP$ , M_WRB , M_XX , M__SP , M_dblSpecialApostrophe , M_normDollar , M_normDblApostrophe , M_normComma , M_norm-LRB- , M_norm-RRB- , M_normDot , M_normColon , M_normADD , M_normAFX , M_normCC , M_normCD , M_normDT , M_normEX , M_normFW , M_normHYPH , M_normIN , M_normJJ , M_normJJR , M_normJJS , M_normLS , M_normMD , M_normNFP , M_normNN , M_normNNP , M_normNNPS , M_normNNS , M_normPDT , M_normPOS , M_normPRP , M_normPRP$ , M_normRB , M_normRBR , M_normRBS , M_normRP , M_normSYM , M_normTO , M_normUH , M_normVB , M_normVBD , M_normVBG , M_normVBN , M_normVBP , M_normVBZ , M_normWDT , M_normWP , M_normWP$ , M_normWRB , M_normXX , M_norm_SP , M_normDblSpecialApostrophe , L_WC , L_Analytic , L_Clout , L_Authentic , L_Tone , L_WPS , L_Sixltr , L_Dic , L_function , L_pronoun , L_ppron , L_i , L_we , L_you , L_shehe , L_they , L_ipron , L_article , L_prep , L_auxverb , L_adverb , L_conj , L_negate , L_verb , L_adj , L_compare , L_interrog , L_number , L_quant , L_affect , L_posemo , L_negemo , L_anx , L_anger , L_sad , L_social , L_family , L_friend , L_female , L_male , L_cogproc , L_insight , L_cause , L_discrep , L_tentat , L_certain , L_differ , L_percept , L_see , L_hear , L_feel , L_bio , L_body , L_health , L_sexual , L_ingest , L_drives , L_affiliation , L_achieve , L_power , L_reward , L_risk , L_focuspast , L_focuspresent , L_focusfuture , L_relativ , L_motion , L_space , L_time , L_work , L_leisure , L_home , L_money , L_relig , L_death , L_informal , L_swear , L_netspeak , L_assent , L_nonflu , L_filler , L_AllPunc , L_Period , L_Comma , L_Colon , L_SemiC , L_QMark , L_Exclam , L_Dash , L_Quote , L_Apostro , L_Parenth , L_OtherP , L_norm_WC , L_norm_Analytic , L_norm_Clout , L_norm_Authentic , L_norm_Tone , L_norm_WPS , L_norm_Sixltr , L_norm_Dic , L_norm_function , L_norm_pronoun , L_norm_ppron , L_norm_i , L_norm_we , L_norm_you , L_norm_shehe , L_norm_they , L_norm_ipron , L_norm_article , L_norm_prep , L_norm_auxverb , L_norm_adverb , L_norm_conj , L_norm_negate , L_norm_verb , L_norm_adj , L_norm_compare , L_norm_interrog , L_norm_number , L_norm_quant , L_norm_affect , L_norm_posemo , L_norm_negemo , L_norm_anx , L_norm_anger , L_norm_sad , L_norm_social , L_norm_family , L_norm_friend , L_norm_female , L_norm_male , L_norm_cogproc , L_norm_insight , L_norm_cause , L_norm_discrep , L_norm_tentat , L_norm_certain , L_norm_differ , L_norm_percept , L_norm_see , L_norm_hear , L_norm_feel , L_norm_bio , L_norm_body , L_norm_health , L_norm_sexual , L_norm_ingest , L_norm_drives , L_norm_affiliation , L_norm_achieve , L_norm_power , L_norm_reward , L_norm_risk , L_norm_focuspast , L_norm_focuspresent , L_norm_focusfuture , L_norm_relativ , L_norm_motion , L_norm_space , L_norm_time , L_norm_work , L_norm_leisure , L_norm_home , L_norm_money , L_norm_relig , L_norm_death , L_norm_informal , L_norm_swear , L_norm_netspeak , L_norm_assent , L_norm_nonflu , L_norm_filler , L_norm_AllPunc , L_norm_Period , L_norm_Comma , L_norm_Colon , L_norm_SemiC , L_norm_QMark , L_norm_Exclam , L_norm_Dash , L_norm_Quote , L_norm_Apostro , L_norm_Parenth , L_norm_OtherP , R_full_text_Flesch_reading_ease , R_full_text_Flesch_kincaid_grade , R_full_text_smog , R_full_text_gunning_fog , R_full_text_words_per_sentence , R_full_text_capitalized_words , R_full_text_lexicon , R_full_text_urls_counts , R_full_text_long_words , R_full_text_syllables , R_full_text_stop_words , R_full_text_sentences , R_full_text_linsear_write , R_full_text_automated_readability , R_full_text_characters_total , R_full_text_coleman_liax , R_full_text_difficult_words , R_full_text_words_total , R_norm_full_text_Flesch_reading_ease , R_norm_full_text_Flesch_kincaid_grade , R_norm_full_text_smog , R_norm_full_text_gunning_fog , R_norm_full_text_words_per_sentence , R_norm_full_text_capitalized_words , R_norm_full_text_lexicon , R_norm_full_text_urls_counts , R_norm_full_text_long_words , R_norm_full_text_syllables , R_norm_full_text_stop_words , R_norm_full_text_sentences , R_norm_full_text_linsear_write , R_norm_full_text_automated_readability , R_norm_full_text_characters_total , R_norm_full_text_coleman_liax , R_norm_full_text_difficult_words , R_norm_full_text_words_total , \n",
      "\n",
      "df_MLR_Out_NO_norm features =\n",
      "domain , url , urlType , M_dollar , M_dblApostrophe , M_comma , M_-LRB- , M_-RRB- , M_dot , M_colon , M_ADD , M_AFX , M_CC , M_CD , M_DT , M_EX , M_FW , M_HYPH , M_IN , M_JJ , M_JJR , M_JJS , M_LS , M_MD , M_NFP , M_NN , M_NNP , M_NNPS , M_NNS , M_PDT , M_POS , M_PRP , M_PRP$ , M_RB , M_RBR , M_RBS , M_RP , M_SYM , M_TO , M_UH , M_VB , M_VBD , M_VBG , M_VBN , M_VBP , M_VBZ , M_WDT , M_WP , M_WP$ , M_WRB , M_XX , M__SP , M_dblSpecialApostrophe , L_WC , L_Analytic , L_Clout , L_Authentic , L_Tone , L_WPS , L_Sixltr , L_Dic , L_function , L_pronoun , L_ppron , L_i , L_we , L_you , L_shehe , L_they , L_ipron , L_article , L_prep , L_auxverb , L_adverb , L_conj , L_negate , L_verb , L_adj , L_compare , L_interrog , L_number , L_quant , L_affect , L_posemo , L_negemo , L_anx , L_anger , L_sad , L_social , L_family , L_friend , L_female , L_male , L_cogproc , L_insight , L_cause , L_discrep , L_tentat , L_certain , L_differ , L_percept , L_see , L_hear , L_feel , L_bio , L_body , L_health , L_sexual , L_ingest , L_drives , L_affiliation , L_achieve , L_power , L_reward , L_risk , L_focuspast , L_focuspresent , L_focusfuture , L_relativ , L_motion , L_space , L_time , L_work , L_leisure , L_home , L_money , L_relig , L_death , L_informal , L_swear , L_netspeak , L_assent , L_nonflu , L_filler , L_AllPunc , L_Period , L_Comma , L_Colon , L_SemiC , L_QMark , L_Exclam , L_Dash , L_Quote , L_Apostro , L_Parenth , L_OtherP , R_full_text_Flesch_reading_ease , R_full_text_Flesch_kincaid_grade , R_full_text_smog , R_full_text_gunning_fog , R_full_text_words_per_sentence , R_full_text_capitalized_words , R_full_text_lexicon , R_full_text_urls_counts , R_full_text_long_words , R_full_text_syllables , R_full_text_stop_words , R_full_text_sentences , R_full_text_linsear_write , R_full_text_automated_readability , R_full_text_characters_total , R_full_text_coleman_liax , R_full_text_difficult_words , R_full_text_words_total , \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# identify all MLR features and write to CSV file\n",
    "colsList_MLR_WITH_norm = []\n",
    "for col in list(df_ALL_Out_WITH_norm) :\n",
    "    if col in ['url' , 'domain' , 'urlType']: # these three must be retained\n",
    "        colsList_MLR_WITH_norm.append(col)\n",
    "    elif col.startswith('M_') or col.startswith('L_') or col.startswith('R_'):\n",
    "        colsList_MLR_WITH_norm.append(col)\n",
    "#\n",
    "df_MLR_Out_WITH_norm = df_ALL_Out_WITH_norm[colsList_MLR_WITH_norm]\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#\n",
    "# check the columns\n",
    "#\n",
    "outColStr = \"\"\n",
    "for col in list(df_MLR_Out_WITH_norm) :\n",
    "    outColStr += col + ' , '\n",
    "print(f\"df_MLR_Out_WITH_norm features =\\n{outColStr}\\n\")\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#\n",
    "#\n",
    "########\n",
    "#\n",
    "# identify MLR features which are regular by dropping normalized features and write to CSV file\n",
    "colsList_MLR_NO_norm = []\n",
    "for col in list(df_ALL_Out_WITH_norm) :\n",
    "    if col in ['url' , 'domain' , 'urlType']: # these three must be retained\n",
    "        colsList_MLR_NO_norm.append(col)\n",
    "    elif col.startswith('M_') or col.startswith('L_') or col.startswith('R_') :\n",
    "        if \"norm\" not in col :\n",
    "            colsList_MLR_NO_norm.append(col)\n",
    "#\n",
    "df_MLR_Out_NO_norm = df_ALL_Out_WITH_norm[colsList_MLR_NO_norm]\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#\n",
    "# check the columns\n",
    "#\n",
    "outColStr = \"\"\n",
    "for col in list(df_MLR_Out_NO_norm) :\n",
    "    outColStr += col + ' , '\n",
    "print(f\"df_MLR_Out_NO_norm features =\\n{outColStr}\\n\")\n",
    "#\n",
    "######## temp code  ###   temp code   ###\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_MLR_Out_WITH_norm.to_csv(folderPath + 'MLR_Features_WITH_norm.csv' , index = True)\n",
    "del df_MLR_Out_WITH_norm\n",
    "df_MLR_Out_NO_norm.to_csv(folderPath + 'MLR_Features_NO_norm.csv' , index = True)\n",
    "del df_MLR_Out_NO_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now all the output files with features are ready:\n",
    "### For WITH normal : one each for All, M, L, T, R, MLR = 6 files\n",
    "### For NO   normal : one each for All, M, L, T, R, MLR = 6 files\n",
    "### Thus total 12 files\n",
    "## ------------------------------------------------------------------------\n",
    "# We will be using the NO normal versions for testing the accuracy of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "####################################################\n",
    "####################################################\n",
    "####################################################\n",
    "####################################################\n",
    "####################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This data can now be used either for training of classifiers or to use as new data to present to already trained models pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/save-load-machine-learning-models-python-scikit-learn/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
